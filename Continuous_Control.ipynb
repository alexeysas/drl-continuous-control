{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "## 1. Explore the Environment\n",
    "\n",
    "First part of the notebook introduces Unity Reacher enviroment and lets you explore its state and actions spaces\n",
    "\n",
    "### 1.1. Install the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Create Unity environment wrapper\n",
    "We need to create Environment wrapper which allows our agent to communicate with Unity environment in an easy way. Just simplifying interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "class EnvironmentWrapper():\n",
    "    def __init__(self, file_name):\n",
    "        self.env = UnityEnvironment(file_name=file_name, worker_id=1)\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        \n",
    "        self.env_info = self.env.reset(train_mode=False)[self.brain_name]\n",
    "        states = self.env_info.vector_observations\n",
    "        self.state_size = states.shape[1]\n",
    "        \n",
    "    def reset(self, train_mode=False):\n",
    "        self.env_info = self.env.reset(train_mode=train_mode)[self.brain_name]\n",
    "        state = self.env_info.vector_observations\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def step(self, actions):\n",
    "        self.env_info = self.env.step(actions)[self.brain_name]       \n",
    "        next_state = self.env_info.vector_observations   \n",
    "        reward = self.env_info.rewards\n",
    "        #reward = [x * 10.0 for x in reward]\n",
    "        done = self.env_info.local_done                \n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load single agent environemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = EnvironmentWrapper(file_name=\"reacher/reacher.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Explore the Environemnt\n",
    "\n",
    "Show environment state and action sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print (env.action_size)\n",
    "print (env.state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='reacher-m/reacher.exe', )\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.env.brain_names[0]\n",
    "brain = env.env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.30408478e+00 -1.00000000e+00\n",
      " -4.92529202e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.33014059e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train DDPG based Agent\n",
    "\n",
    "Now it's time to train agent to solve the environment! When training the environment. We implemented DDPG algorithm described in the paper: https://arxiv.org/abs/1509.02971 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Import PyTorch\n",
    "First we need to import PyTorch library so we can start building two networks:  Policy Network (Actor) to generate actions an Q-Network (Critic) to calculate Q value of generated action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. Setup Network Archtiectures\n",
    "For the Policy network (Actor) we ended up with simple three layers fully connected network with Relu as activation functions and Batch Normalization  For Q-Network (Critic) we come up with four layers fully connected network with Relu as activation functions.  Dropout regularization is also used to improve Network generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.state_size =  state_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.state_size, 192)\n",
    "        self.batch_norm1 =  torch.nn.BatchNorm1d(192)\n",
    "        self.fc2 = nn.Linear(192, 96)\n",
    "        self.batch_norm2 =  torch.nn.BatchNorm1d(96)\n",
    "        self.fc3 = nn.Linear(96, self.action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # Layer 1\n",
    "        x = self.fc1(state)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = F.tanh(self.fc3(x))     \n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Q-Network) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.state_size =  state_size\n",
    "        \n",
    "        self.fc1s = nn.Linear(self.state_size, 128)\n",
    "        self.fc2 = nn.Linear(128 + self.action_size, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "                \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # State feature layer\n",
    "        xs = F.relu(self.fc1s(state))\n",
    "        \n",
    "        # State freatures + action values\n",
    "        x =  torch.cat((xs, action), dim=1)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 4\n",
    "        x = self.fc4(x)     \n",
    "         \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Create Expirience Replay Buffer\n",
    "We need to define class with will be used to store experience tuples. DDPG algorithm also uses previous expirience similar to the DQN for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Create Ornstein-Uhlenbeck process class\n",
    "\n",
    "As was mentioned in the original paper a major challenge of learning in continuous action spaces is exploration. So to make agent to explore we are adding some noise to the actions using an Ornstein-Uhlenbeck process.  So we need to define noise class to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Create agent\n",
    "\n",
    "Create DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# hyper params sample\n",
    "hyperparams = { \"BUFFER_SIZE\" : int(1e5),  # replay buffer size\n",
    "                \"BATCH_SIZE\" : 64,         # minibatch size\n",
    "                \"GAMMA\" : 0.99,            # discount factor\n",
    "                \"TAU\" : 1e-3,              # for soft update of target parameters\n",
    "                \"LR\" : 1e-3,               # learning rate \n",
    "                \"LEARN_EVERY\" : 4,         # how often to update the networks\n",
    "                \"LEARN_ITERATIONS\" : 10,   # how many iterations needed for each network update\n",
    "              }\n",
    "\n",
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, device, hyperparams, actor, critic, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an DDPG Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.hyperparams = hyperparams\n",
    "        \n",
    "        self.device = device\n",
    "           \n",
    "        self.BUFFER_SIZE = hyperparams[\"BUFFER_SIZE\"]\n",
    "        self.BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "        self.GAMMA = hyperparams[\"GAMMA\"]\n",
    "        self.TAU = hyperparams[\"TAU\"]\n",
    "        self.LR = hyperparams[\"LR\"]\n",
    "        \n",
    "        self.LEARN_EVERY = hyperparams[\"LEARN_EVERY\"]\n",
    "        self.LEARN_ITERATIONS = hyperparams[\"LEARN_ITERATIONS\"]\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Actor\n",
    "        self.actor = actor(state_size, action_size, seed).to(self.device)\n",
    "        self.actor_target = actor(state_size, action_size, seed).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.LR)\n",
    "        self.actor.eval()\n",
    "       \n",
    "         # Critic\n",
    "        self.critic = critic(state_size, action_size, seed).to(self.device)\n",
    "        self.critic_target = critic(state_size, action_size, seed).to(self.device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.LR)\n",
    "        self.critic.train()\n",
    "        \n",
    "        # Deep Q-Learning Algorithm: Initialize replay memory D with N = BUFFER_SIZE\n",
    "        self.memory = ReplayBuffer(action_size, self.BUFFER_SIZE, self.BATCH_SIZE, seed)\n",
    "       \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "         # Noise process\n",
    "        self.noise = OUNoise(action_size, seed)\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience tuple (S, A, R, S~) in replay memory D\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.LEARN_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in the memory, run learning cycle\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                # run learning cycle LEARN_ITERATIONS times \n",
    "                for _ in range(self.LEARN_ITERATIONS):\n",
    "                    # Obtain random minibatch of tuples (S, A, R, S~) from replay memory D\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, self.GAMMA)\n",
    "                    \n",
    "                    # update target networks using soft update\n",
    "                    self.soft_update(self.actor, self.actor_target, self.TAU)   \n",
    "                    self.soft_update(self.critic, self.critic_target, self.TAU)   \n",
    "    \n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        \n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        self.actor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_values = self.actor(states).cpu().data.numpy()\n",
    "        self.actor.train()\n",
    "        \n",
    "        # add noise to the actions to favor exploartion\n",
    "        if add_noise:\n",
    "            action_values += self.noise.sample() / 10.0\n",
    "        \n",
    "        # make sure that resulting value after adding noise is still in [-1, 1] interval\n",
    "        return np.clip(action_values, -1.0, 1.0)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use random minibatch of tuples(S[j], A[j], R[j], S[j+1]) from replay memory D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        with torch.no_grad():            \n",
    "            # get continious actions vector according to the target policy network (Actor)\n",
    "            # for the next states\n",
    "            actions_next = self.actor_target(next_states)\n",
    "            \n",
    "            # evaluate q values for the actions proposed by policy network for the next states\n",
    "            # Q(states_next, actions_next) by target Q network (Critic)\n",
    "            Q_states_next_actions_next = self.critic_target(next_states,  actions_next)\n",
    "                       \n",
    "            # calculate target value\n",
    "            # y[j]= r[j] if episode terminates (done = 1)\n",
    "            y = rewards + gamma * Q_states_next_actions_next * (1.0 - dones)\n",
    "        \n",
    "        # get expected Q value \n",
    "        Q_expected = self.critic(states, actions)\n",
    "        \n",
    "        # critic loss is normal MSE loss, we need to minimize\n",
    "        # difference between actual Q values and target Q values for specific state, action pairs\n",
    "        critic_loss = F.mse_loss(Q_expected, y)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic.parameters(), 1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # predict new actions according to current policy network\n",
    "        predicted_actions = self.actor(states)\n",
    "        \n",
    "        # we need to maximize Q values calculated by critic based on predicted actions,\n",
    "        # so we need to perform gradient ascent (minus sign)\n",
    "        actor_loss = -self.critic(states, predicted_actions).mean()\n",
    "        \n",
    "        # backpropagation\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.noise.reset()\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Setup training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=100, max_t=1000, model_file_actor = 'actor.pth', model_file_critic = 'critic.pth'):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    all_scores = []                     # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)   # last 100 scores\n",
    "    scores_window10 = deque(maxlen=10)  # last 10 scores\n",
    "    max_score = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # Not sure why - sometimes unity environment is not reset after first call.\n",
    "        # so call reset two times - works like a charm\n",
    "        states = env.reset(train_mode=True)\n",
    "        states = env.reset(train_mode=True)\n",
    "        \n",
    "        # reset scores for the episode\n",
    "        scores = np.zeros(states.shape[0], dtype=float)\n",
    "        \n",
    "        # for time step t <- 1 to T:\n",
    "        for t in range(max_t):\n",
    "           \n",
    "            # Choose action from state S using policy network:\n",
    "            actions = agent.act(states)\n",
    "        \n",
    "            # Take an action and observe result\n",
    "            next_states, rewards, dones = env.step(actions)\n",
    "         \n",
    "            # Sample and Learn section\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break \n",
    "      \n",
    "        mean_score = np.mean(scores)\n",
    "        scores_window10.append(mean_score)\n",
    "        scores_window.append(mean_score)       # save most recent score\n",
    "        all_scores.append(mean_score)              # save most recent score\n",
    "        \n",
    "        print('Episode {}\\t Episode score: {:.3f} \\t Mean-10 score: {:.3f} \\t Mean-100 score: {:.3f}'\n",
    "              .format(i_episode, mean_score, np.mean(scores_window10), np.mean(scores_window)))\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score over 100 episodes: {:.3f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "            if np.mean(scores_window) > max_score:\n",
    "                print('Saving model... Episode: {} \\tAverage Score: {:.6f}\\n'.format(i_episode, np.mean(scores_window)))\n",
    "                torch.save(agent.actor.state_dict(), model_file_actor)\n",
    "                torch.save(agent.critic.state_dict(), model_file_critic)\n",
    "                max_score = np.mean(scores_window)\n",
    "        \n",
    "        if np.mean(scores_window) > 30.2:\n",
    "                print('Solved! Saving model... Episode: {} \\tAverage Score: {:.6f}\\n'.format(i_episode, np.mean(scores_window)))\n",
    "                torch.save(agent.actor.state_dict(), model_file_actor)\n",
    "                torch.save(agent.critic.state_dict(), model_file_critic)\n",
    "                max_score = np.mean(scores_window)\n",
    "                break\n",
    "                \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Set device and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "    \n",
    "def moving_average(data_set, periods=3):\n",
    "    weights = np.ones(periods) / periods\n",
    "    return np.convolve(data_set, weights, mode='valid')\n",
    "\n",
    "def plot_comparison(models,  labels, periods):\n",
    "    \n",
    "    fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    \n",
    "    for m, l in zip(models, labels):\n",
    "        moving_m =  moving_average(m, periods)\n",
    "        plt.plot(moving_m, label=l)\n",
    "    \n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.title(\"DQN Models comparison\", fontsize=12, fontweight='bold')\n",
    "    plt.xlabel(\"Eisode #\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Episode 1\t Episode score: 0.560 \t Mean-10 score: 0.560 \t Mean-100 score: 0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\ipykernel_launcher.py:147: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\t Episode score: 0.460 \t Mean-10 score: 0.510 \t Mean-100 score: 0.510\n",
      "Episode 3\t Episode score: 0.950 \t Mean-10 score: 0.657 \t Mean-100 score: 0.657\n",
      "Episode 4\t Episode score: 0.720 \t Mean-10 score: 0.672 \t Mean-100 score: 0.672\n",
      "Episode 5\t Episode score: 1.230 \t Mean-10 score: 0.784 \t Mean-100 score: 0.784\n",
      "Episode 6\t Episode score: 1.380 \t Mean-10 score: 0.883 \t Mean-100 score: 0.883\n",
      "Episode 7\t Episode score: 1.820 \t Mean-10 score: 1.017 \t Mean-100 score: 1.017\n",
      "Episode 8\t Episode score: 1.300 \t Mean-10 score: 1.052 \t Mean-100 score: 1.052\n",
      "Episode 9\t Episode score: 1.380 \t Mean-10 score: 1.089 \t Mean-100 score: 1.089\n",
      "Episode 10\t Episode score: 1.310 \t Mean-10 score: 1.111 \t Mean-100 score: 1.111\n",
      "Episode 11\t Episode score: 1.430 \t Mean-10 score: 1.198 \t Mean-100 score: 1.140\n",
      "Episode 12\t Episode score: 1.910 \t Mean-10 score: 1.343 \t Mean-100 score: 1.204\n",
      "Episode 13\t Episode score: 1.810 \t Mean-10 score: 1.429 \t Mean-100 score: 1.251\n",
      "Episode 14\t Episode score: 1.070 \t Mean-10 score: 1.464 \t Mean-100 score: 1.238\n",
      "Episode 15\t Episode score: 1.520 \t Mean-10 score: 1.493 \t Mean-100 score: 1.257\n",
      "Episode 16\t Episode score: 1.860 \t Mean-10 score: 1.541 \t Mean-100 score: 1.294\n",
      "Episode 17\t Episode score: 1.680 \t Mean-10 score: 1.527 \t Mean-100 score: 1.317\n",
      "Episode 18\t Episode score: 1.400 \t Mean-10 score: 1.537 \t Mean-100 score: 1.322\n",
      "Episode 19\t Episode score: 1.840 \t Mean-10 score: 1.583 \t Mean-100 score: 1.349\n",
      "Episode 20\t Episode score: 0.960 \t Mean-10 score: 1.548 \t Mean-100 score: 1.329\n",
      "Episode 21\t Episode score: 0.830 \t Mean-10 score: 1.488 \t Mean-100 score: 1.306\n",
      "Episode 22\t Episode score: 2.580 \t Mean-10 score: 1.555 \t Mean-100 score: 1.364\n",
      "Episode 23\t Episode score: 2.340 \t Mean-10 score: 1.608 \t Mean-100 score: 1.406\n",
      "Episode 24\t Episode score: 0.980 \t Mean-10 score: 1.599 \t Mean-100 score: 1.388\n",
      "Episode 25\t Episode score: 1.250 \t Mean-10 score: 1.572 \t Mean-100 score: 1.383\n",
      "Episode 26\t Episode score: 0.940 \t Mean-10 score: 1.480 \t Mean-100 score: 1.366\n",
      "Episode 27\t Episode score: 0.150 \t Mean-10 score: 1.327 \t Mean-100 score: 1.321\n",
      "Episode 28\t Episode score: 1.430 \t Mean-10 score: 1.330 \t Mean-100 score: 1.325\n",
      "Episode 29\t Episode score: 1.560 \t Mean-10 score: 1.302 \t Mean-100 score: 1.333\n",
      "Episode 30\t Episode score: 2.630 \t Mean-10 score: 1.469 \t Mean-100 score: 1.376\n",
      "Episode 31\t Episode score: 3.210 \t Mean-10 score: 1.707 \t Mean-100 score: 1.435\n",
      "Episode 32\t Episode score: 4.160 \t Mean-10 score: 1.865 \t Mean-100 score: 1.520\n",
      "Episode 33\t Episode score: 1.230 \t Mean-10 score: 1.754 \t Mean-100 score: 1.512\n",
      "Episode 34\t Episode score: 3.360 \t Mean-10 score: 1.992 \t Mean-100 score: 1.566\n",
      "Episode 35\t Episode score: 5.080 \t Mean-10 score: 2.375 \t Mean-100 score: 1.666\n",
      "Episode 36\t Episode score: 4.240 \t Mean-10 score: 2.705 \t Mean-100 score: 1.738\n",
      "Episode 37\t Episode score: 4.170 \t Mean-10 score: 3.107 \t Mean-100 score: 1.804\n",
      "Episode 38\t Episode score: 5.220 \t Mean-10 score: 3.486 \t Mean-100 score: 1.893\n",
      "Episode 39\t Episode score: 2.830 \t Mean-10 score: 3.613 \t Mean-100 score: 1.917\n",
      "Episode 40\t Episode score: 6.570 \t Mean-10 score: 4.007 \t Mean-100 score: 2.034\n",
      "Episode 41\t Episode score: 5.200 \t Mean-10 score: 4.206 \t Mean-100 score: 2.111\n",
      "Episode 42\t Episode score: 10.080 \t Mean-10 score: 4.798 \t Mean-100 score: 2.301\n",
      "Episode 43\t Episode score: 5.210 \t Mean-10 score: 5.196 \t Mean-100 score: 2.368\n",
      "Episode 44\t Episode score: 5.660 \t Mean-10 score: 5.426 \t Mean-100 score: 2.443\n",
      "Episode 45\t Episode score: 5.510 \t Mean-10 score: 5.469 \t Mean-100 score: 2.511\n",
      "Episode 46\t Episode score: 8.330 \t Mean-10 score: 5.878 \t Mean-100 score: 2.638\n",
      "Episode 47\t Episode score: 5.450 \t Mean-10 score: 6.006 \t Mean-100 score: 2.698\n",
      "Episode 48\t Episode score: 5.120 \t Mean-10 score: 5.996 \t Mean-100 score: 2.748\n",
      "Episode 49\t Episode score: 7.910 \t Mean-10 score: 6.504 \t Mean-100 score: 2.853\n",
      "Episode 50\t Episode score: 6.990 \t Mean-10 score: 6.546 \t Mean-100 score: 2.936\n",
      "Episode 51\t Episode score: 5.480 \t Mean-10 score: 6.574 \t Mean-100 score: 2.986\n",
      "Episode 52\t Episode score: 7.360 \t Mean-10 score: 6.302 \t Mean-100 score: 3.070\n",
      "Episode 53\t Episode score: 6.870 \t Mean-10 score: 6.468 \t Mean-100 score: 3.142\n",
      "Episode 54\t Episode score: 6.120 \t Mean-10 score: 6.514 \t Mean-100 score: 3.197\n",
      "Episode 55\t Episode score: 7.730 \t Mean-10 score: 6.736 \t Mean-100 score: 3.279\n",
      "Episode 56\t Episode score: 10.410 \t Mean-10 score: 6.944 \t Mean-100 score: 3.407\n",
      "Episode 57\t Episode score: 10.070 \t Mean-10 score: 7.406 \t Mean-100 score: 3.524\n",
      "Episode 58\t Episode score: 6.470 \t Mean-10 score: 7.541 \t Mean-100 score: 3.574\n",
      "Episode 59\t Episode score: 3.710 \t Mean-10 score: 7.121 \t Mean-100 score: 3.577\n",
      "Episode 60\t Episode score: 12.060 \t Mean-10 score: 7.628 \t Mean-100 score: 3.718\n",
      "Episode 61\t Episode score: 10.440 \t Mean-10 score: 8.124 \t Mean-100 score: 3.828\n",
      "Episode 62\t Episode score: 5.090 \t Mean-10 score: 7.897 \t Mean-100 score: 3.849\n",
      "Episode 63\t Episode score: 5.170 \t Mean-10 score: 7.727 \t Mean-100 score: 3.870\n",
      "Episode 64\t Episode score: 4.890 \t Mean-10 score: 7.604 \t Mean-100 score: 3.886\n",
      "Episode 65\t Episode score: 14.570 \t Mean-10 score: 8.288 \t Mean-100 score: 4.050\n",
      "Episode 66\t Episode score: 4.950 \t Mean-10 score: 7.742 \t Mean-100 score: 4.064\n",
      "Episode 67\t Episode score: 11.320 \t Mean-10 score: 7.867 \t Mean-100 score: 4.172\n",
      "Episode 68\t Episode score: 3.970 \t Mean-10 score: 7.617 \t Mean-100 score: 4.169\n",
      "Episode 69\t Episode score: 9.020 \t Mean-10 score: 8.148 \t Mean-100 score: 4.239\n",
      "Episode 70\t Episode score: 7.140 \t Mean-10 score: 7.656 \t Mean-100 score: 4.281\n",
      "Episode 71\t Episode score: 10.860 \t Mean-10 score: 7.698 \t Mean-100 score: 4.373\n",
      "Episode 72\t Episode score: 10.860 \t Mean-10 score: 8.275 \t Mean-100 score: 4.463\n",
      "Episode 73\t Episode score: 7.210 \t Mean-10 score: 8.479 \t Mean-100 score: 4.501\n",
      "Episode 74\t Episode score: 5.540 \t Mean-10 score: 8.544 \t Mean-100 score: 4.515\n",
      "Episode 75\t Episode score: 10.220 \t Mean-10 score: 8.109 \t Mean-100 score: 4.591\n",
      "Episode 76\t Episode score: 9.250 \t Mean-10 score: 8.539 \t Mean-100 score: 4.652\n",
      "Episode 77\t Episode score: 12.040 \t Mean-10 score: 8.611 \t Mean-100 score: 4.748\n",
      "Episode 78\t Episode score: 12.250 \t Mean-10 score: 9.439 \t Mean-100 score: 4.845\n",
      "Episode 79\t Episode score: 8.070 \t Mean-10 score: 9.344 \t Mean-100 score: 4.885\n",
      "Episode 80\t Episode score: 8.900 \t Mean-10 score: 9.520 \t Mean-100 score: 4.936\n",
      "Episode 81\t Episode score: 9.200 \t Mean-10 score: 9.354 \t Mean-100 score: 4.988\n",
      "Episode 82\t Episode score: 8.470 \t Mean-10 score: 9.115 \t Mean-100 score: 5.031\n",
      "Episode 83\t Episode score: 14.480 \t Mean-10 score: 9.842 \t Mean-100 score: 5.145\n",
      "Episode 84\t Episode score: 11.590 \t Mean-10 score: 10.447 \t Mean-100 score: 5.221\n",
      "Episode 85\t Episode score: 6.580 \t Mean-10 score: 10.083 \t Mean-100 score: 5.237\n",
      "Episode 86\t Episode score: 8.550 \t Mean-10 score: 10.013 \t Mean-100 score: 5.276\n",
      "Episode 87\t Episode score: 13.940 \t Mean-10 score: 10.203 \t Mean-100 score: 5.375\n",
      "Episode 88\t Episode score: 16.580 \t Mean-10 score: 10.636 \t Mean-100 score: 5.503\n",
      "Episode 89\t Episode score: 21.180 \t Mean-10 score: 11.947 \t Mean-100 score: 5.679\n",
      "Episode 90\t Episode score: 13.680 \t Mean-10 score: 12.425 \t Mean-100 score: 5.768\n",
      "Episode 91\t Episode score: 12.480 \t Mean-10 score: 12.753 \t Mean-100 score: 5.842\n",
      "Episode 92\t Episode score: 11.190 \t Mean-10 score: 13.025 \t Mean-100 score: 5.900\n",
      "Episode 93\t Episode score: 16.630 \t Mean-10 score: 13.240 \t Mean-100 score: 6.015\n",
      "Episode 94\t Episode score: 7.680 \t Mean-10 score: 12.849 \t Mean-100 score: 6.033\n",
      "Episode 95\t Episode score: 23.790 \t Mean-10 score: 14.570 \t Mean-100 score: 6.220\n",
      "Episode 96\t Episode score: 16.480 \t Mean-10 score: 15.363 \t Mean-100 score: 6.327\n",
      "Episode 97\t Episode score: 12.720 \t Mean-10 score: 15.241 \t Mean-100 score: 6.392\n",
      "Episode 98\t Episode score: 14.010 \t Mean-10 score: 14.984 \t Mean-100 score: 6.470\n",
      "Episode 99\t Episode score: 22.970 \t Mean-10 score: 15.163 \t Mean-100 score: 6.637\n",
      "Episode 100\t Episode score: 15.570 \t Mean-10 score: 15.352 \t Mean-100 score: 6.726\n",
      "Episode 100\tAverage Score over 100 episodes: 6.726\n",
      "Saving model... Episode: 100 \tAverage Score: 6.726200\n",
      "\n",
      "Episode 101\t Episode score: 15.430 \t Mean-10 score: 15.647 \t Mean-100 score: 6.875\n",
      "Episode 102\t Episode score: 20.670 \t Mean-10 score: 16.595 \t Mean-100 score: 7.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 103\t Episode score: 19.910 \t Mean-10 score: 16.923 \t Mean-100 score: 7.267\n",
      "Episode 104\t Episode score: 12.010 \t Mean-10 score: 17.356 \t Mean-100 score: 7.379\n",
      "Episode 105\t Episode score: 17.390 \t Mean-10 score: 16.716 \t Mean-100 score: 7.541\n",
      "Episode 106\t Episode score: 20.370 \t Mean-10 score: 17.105 \t Mean-100 score: 7.731\n",
      "Episode 107\t Episode score: 16.460 \t Mean-10 score: 17.479 \t Mean-100 score: 7.877\n",
      "Episode 108\t Episode score: 23.960 \t Mean-10 score: 18.474 \t Mean-100 score: 8.104\n",
      "Episode 109\t Episode score: 19.200 \t Mean-10 score: 18.097 \t Mean-100 score: 8.282\n",
      "Episode 110\t Episode score: 19.900 \t Mean-10 score: 18.530 \t Mean-100 score: 8.468\n",
      "Episode 111\t Episode score: 11.480 \t Mean-10 score: 18.135 \t Mean-100 score: 8.569\n",
      "Episode 112\t Episode score: 20.260 \t Mean-10 score: 18.094 \t Mean-100 score: 8.752\n",
      "Episode 113\t Episode score: 24.040 \t Mean-10 score: 18.507 \t Mean-100 score: 8.974\n",
      "Episode 114\t Episode score: 22.310 \t Mean-10 score: 19.537 \t Mean-100 score: 9.187\n",
      "Episode 115\t Episode score: 18.760 \t Mean-10 score: 19.674 \t Mean-100 score: 9.359\n",
      "Episode 116\t Episode score: 24.290 \t Mean-10 score: 20.066 \t Mean-100 score: 9.583\n",
      "Episode 117\t Episode score: 22.290 \t Mean-10 score: 20.649 \t Mean-100 score: 9.790\n",
      "Episode 118\t Episode score: 20.460 \t Mean-10 score: 20.299 \t Mean-100 score: 9.980\n",
      "Episode 119\t Episode score: 19.630 \t Mean-10 score: 20.342 \t Mean-100 score: 10.158\n",
      "Episode 120\t Episode score: 26.680 \t Mean-10 score: 21.020 \t Mean-100 score: 10.415\n",
      "Episode 121\t Episode score: 23.530 \t Mean-10 score: 22.225 \t Mean-100 score: 10.642\n",
      "Episode 122\t Episode score: 28.090 \t Mean-10 score: 23.008 \t Mean-100 score: 10.897\n",
      "Episode 123\t Episode score: 20.230 \t Mean-10 score: 22.627 \t Mean-100 score: 11.076\n",
      "Episode 124\t Episode score: 29.490 \t Mean-10 score: 23.345 \t Mean-100 score: 11.361\n",
      "Episode 125\t Episode score: 27.900 \t Mean-10 score: 24.259 \t Mean-100 score: 11.628\n",
      "Episode 126\t Episode score: 26.140 \t Mean-10 score: 24.444 \t Mean-100 score: 11.880\n",
      "Episode 127\t Episode score: 30.980 \t Mean-10 score: 25.313 \t Mean-100 score: 12.188\n",
      "Episode 128\t Episode score: 16.090 \t Mean-10 score: 24.876 \t Mean-100 score: 12.335\n",
      "Episode 129\t Episode score: 26.480 \t Mean-10 score: 25.561 \t Mean-100 score: 12.584\n",
      "Episode 130\t Episode score: 26.880 \t Mean-10 score: 25.581 \t Mean-100 score: 12.826\n",
      "Episode 131\t Episode score: 19.500 \t Mean-10 score: 25.178 \t Mean-100 score: 12.989\n",
      "Episode 132\t Episode score: 24.330 \t Mean-10 score: 24.802 \t Mean-100 score: 13.191\n",
      "Episode 133\t Episode score: 26.460 \t Mean-10 score: 25.425 \t Mean-100 score: 13.443\n",
      "Episode 134\t Episode score: 30.990 \t Mean-10 score: 25.575 \t Mean-100 score: 13.720\n",
      "Episode 135\t Episode score: 13.240 \t Mean-10 score: 24.109 \t Mean-100 score: 13.801\n",
      "Episode 136\t Episode score: 25.360 \t Mean-10 score: 24.031 \t Mean-100 score: 14.012\n",
      "Episode 137\t Episode score: 31.130 \t Mean-10 score: 24.046 \t Mean-100 score: 14.282\n",
      "Episode 138\t Episode score: 21.070 \t Mean-10 score: 24.544 \t Mean-100 score: 14.441\n",
      "Episode 139\t Episode score: 35.520 \t Mean-10 score: 25.448 \t Mean-100 score: 14.767\n",
      "Episode 140\t Episode score: 29.300 \t Mean-10 score: 25.690 \t Mean-100 score: 14.995\n",
      "Episode 141\t Episode score: 32.440 \t Mean-10 score: 26.984 \t Mean-100 score: 15.267\n",
      "Episode 142\t Episode score: 33.960 \t Mean-10 score: 27.947 \t Mean-100 score: 15.506\n",
      "Episode 143\t Episode score: 32.770 \t Mean-10 score: 28.578 \t Mean-100 score: 15.782\n",
      "Episode 144\t Episode score: 31.700 \t Mean-10 score: 28.649 \t Mean-100 score: 16.042\n",
      "Episode 145\t Episode score: 24.510 \t Mean-10 score: 29.776 \t Mean-100 score: 16.232\n",
      "Episode 146\t Episode score: 36.080 \t Mean-10 score: 30.848 \t Mean-100 score: 16.509\n",
      "Episode 147\t Episode score: 25.740 \t Mean-10 score: 30.309 \t Mean-100 score: 16.712\n",
      "Episode 148\t Episode score: 35.170 \t Mean-10 score: 31.719 \t Mean-100 score: 17.013\n",
      "Episode 149\t Episode score: 39.520 \t Mean-10 score: 32.119 \t Mean-100 score: 17.329\n",
      "Episode 150\t Episode score: 27.780 \t Mean-10 score: 31.967 \t Mean-100 score: 17.537\n",
      "Episode 151\t Episode score: 37.120 \t Mean-10 score: 32.435 \t Mean-100 score: 17.853\n",
      "Episode 152\t Episode score: 26.000 \t Mean-10 score: 31.639 \t Mean-100 score: 18.040\n",
      "Episode 153\t Episode score: 36.400 \t Mean-10 score: 32.002 \t Mean-100 score: 18.335\n",
      "Episode 154\t Episode score: 31.460 \t Mean-10 score: 31.978 \t Mean-100 score: 18.588\n",
      "Episode 155\t Episode score: 33.110 \t Mean-10 score: 32.838 \t Mean-100 score: 18.842\n",
      "Episode 156\t Episode score: 37.430 \t Mean-10 score: 32.973 \t Mean-100 score: 19.112\n",
      "Episode 157\t Episode score: 32.780 \t Mean-10 score: 33.677 \t Mean-100 score: 19.339\n",
      "Episode 158\t Episode score: 34.220 \t Mean-10 score: 33.582 \t Mean-100 score: 19.617\n",
      "Episode 159\t Episode score: 30.150 \t Mean-10 score: 32.645 \t Mean-100 score: 19.881\n",
      "Episode 160\t Episode score: 33.640 \t Mean-10 score: 33.231 \t Mean-100 score: 20.097\n",
      "Episode 161\t Episode score: 35.780 \t Mean-10 score: 33.097 \t Mean-100 score: 20.351\n",
      "Episode 162\t Episode score: 25.200 \t Mean-10 score: 33.017 \t Mean-100 score: 20.552\n",
      "Episode 163\t Episode score: 38.860 \t Mean-10 score: 33.263 \t Mean-100 score: 20.889\n",
      "Episode 164\t Episode score: 35.420 \t Mean-10 score: 33.659 \t Mean-100 score: 21.194\n",
      "Episode 165\t Episode score: 38.220 \t Mean-10 score: 34.170 \t Mean-100 score: 21.430\n",
      "Episode 166\t Episode score: 36.440 \t Mean-10 score: 34.071 \t Mean-100 score: 21.745\n",
      "Episode 167\t Episode score: 28.320 \t Mean-10 score: 33.625 \t Mean-100 score: 21.915\n",
      "Episode 168\t Episode score: 37.350 \t Mean-10 score: 33.938 \t Mean-100 score: 22.249\n",
      "Episode 169\t Episode score: 37.870 \t Mean-10 score: 34.710 \t Mean-100 score: 22.538\n",
      "Episode 170\t Episode score: 36.180 \t Mean-10 score: 34.964 \t Mean-100 score: 22.828\n",
      "Episode 171\t Episode score: 36.760 \t Mean-10 score: 35.062 \t Mean-100 score: 23.087\n",
      "Episode 172\t Episode score: 36.950 \t Mean-10 score: 36.237 \t Mean-100 score: 23.348\n",
      "Episode 173\t Episode score: 37.450 \t Mean-10 score: 36.096 \t Mean-100 score: 23.650\n",
      "Episode 174\t Episode score: 32.800 \t Mean-10 score: 35.834 \t Mean-100 score: 23.923\n",
      "Episode 175\t Episode score: 34.830 \t Mean-10 score: 35.495 \t Mean-100 score: 24.169\n",
      "Episode 176\t Episode score: 34.150 \t Mean-10 score: 35.266 \t Mean-100 score: 24.418\n",
      "Episode 177\t Episode score: 35.240 \t Mean-10 score: 35.958 \t Mean-100 score: 24.650\n",
      "Episode 178\t Episode score: 35.220 \t Mean-10 score: 35.745 \t Mean-100 score: 24.880\n",
      "Episode 179\t Episode score: 35.180 \t Mean-10 score: 35.476 \t Mean-100 score: 25.151\n",
      "Episode 180\t Episode score: 25.400 \t Mean-10 score: 34.398 \t Mean-100 score: 25.316\n",
      "Episode 181\t Episode score: 34.860 \t Mean-10 score: 34.208 \t Mean-100 score: 25.572\n",
      "Episode 182\t Episode score: 37.370 \t Mean-10 score: 34.250 \t Mean-100 score: 25.861\n",
      "Episode 183\t Episode score: 38.620 \t Mean-10 score: 34.367 \t Mean-100 score: 26.103\n",
      "Episode 184\t Episode score: 39.370 \t Mean-10 score: 35.024 \t Mean-100 score: 26.381\n",
      "Episode 185\t Episode score: 39.510 \t Mean-10 score: 35.492 \t Mean-100 score: 26.710\n",
      "Episode 186\t Episode score: 29.300 \t Mean-10 score: 35.007 \t Mean-100 score: 26.917\n",
      "Episode 187\t Episode score: 39.230 \t Mean-10 score: 35.406 \t Mean-100 score: 27.170\n",
      "Episode 188\t Episode score: 39.420 \t Mean-10 score: 35.826 \t Mean-100 score: 27.399\n",
      "Episode 189\t Episode score: 38.340 \t Mean-10 score: 36.142 \t Mean-100 score: 27.570\n",
      "Episode 190\t Episode score: 37.560 \t Mean-10 score: 37.358 \t Mean-100 score: 27.809\n",
      "Episode 191\t Episode score: 38.910 \t Mean-10 score: 37.763 \t Mean-100 score: 28.073\n",
      "Episode 192\t Episode score: 39.560 \t Mean-10 score: 37.982 \t Mean-100 score: 28.357\n",
      "Episode 193\t Episode score: 36.680 \t Mean-10 score: 37.788 \t Mean-100 score: 28.558\n",
      "Episode 194\t Episode score: 39.010 \t Mean-10 score: 37.752 \t Mean-100 score: 28.871\n",
      "Episode 195\t Episode score: 39.540 \t Mean-10 score: 37.755 \t Mean-100 score: 29.028\n",
      "Episode 196\t Episode score: 39.540 \t Mean-10 score: 38.779 \t Mean-100 score: 29.259\n",
      "Episode 197\t Episode score: 39.310 \t Mean-10 score: 38.787 \t Mean-100 score: 29.525\n",
      "Episode 198\t Episode score: 39.170 \t Mean-10 score: 38.762 \t Mean-100 score: 29.776\n",
      "Episode 199\t Episode score: 34.830 \t Mean-10 score: 38.411 \t Mean-100 score: 29.895\n",
      "Episode 200\t Episode score: 36.770 \t Mean-10 score: 38.332 \t Mean-100 score: 30.107\n",
      "Episode 200\tAverage Score over 100 episodes: 30.107\n",
      "Saving model... Episode: 200 \tAverage Score: 30.107099\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-06df52f99633>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file_actor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DDPG_actor.pth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file_critic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DDPG_critic.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DDPGScores.npy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-c741f3b3b41d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, n_episodes, max_t, eps_start, eps_end, eps_decay, model_file_actor, model_file_critic)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m# Deep Q-Learning algorithm: Finish Sample and full Learn section\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-dc0c1c7cc814>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARN_ITERATIONS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                     \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-4a32facb3aec>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \"\"\"\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparams = { \"BUFFER_SIZE\" : int(1e6),  # replay buffer size\n",
    "                \"BATCH_SIZE\" : 1024,         # minibatch size\n",
    "                \"GAMMA\" : 0.99,             # discount factor\n",
    "                \"TAU\" : 1e-3,               # for soft update of target parameters\n",
    "                \"LR\" : 1e-4,                # learning rate \n",
    "                \"LEARN_EVERY\" : 10,         # how often to update the network\n",
    "                \"LEARN_ITERATIONS\" : 10,    # how many iterations needed for each network update\n",
    "              }\n",
    "\n",
    "agent = DDPGAgent(device, hyperparams, Actor, Critic, state_size=env.state_size, action_size=env.action_size, seed=0)\n",
    "\n",
    "scores = train(env, agent, n_episodes=500, max_t=1000, model_file_actor = 'DDPG_actor.pth', model_file_critic = 'DDPG_critic.pth')\n",
    "\n",
    "np.save(\"DDPGScores.npy\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "aver = moving_average(scores, 10)\n",
    "\n",
    "plot_scores(aver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Load agent and watch performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0\n"
     ]
    }
   ],
   "source": [
    "hyperparams = { \"BUFFER_SIZE\" : int(1e6),  # replay buffer size\n",
    "                \"BATCH_SIZE\" : 1024,         # minibatch size\n",
    "                \"GAMMA\" : 0.99,             # discount factor\n",
    "                \"TAU\" : 1e-3,               # for soft update of target parameters\n",
    "                \"LR\" : 1e-4,                # learning rate \n",
    "                \"LEARN_EVERY\" : 10,         # how often to update the network\n",
    "                \"LEARN_ITERATIONS\" : 10,    # how many iterations needed for each network update\n",
    "              }\n",
    "\n",
    "agent = DDPGAgent(device, hyperparams, Actor, Critic, state_size=env.state_size, action_size=env.action_size, seed=0)\n",
    "\n",
    "actor_state_dict = torch.load('DDPG_actor.pth')\n",
    "critic_state_dict = torch.load('DDPG_critic.pth')\n",
    "\n",
    "agent.actor.load_state_dict(actor_state_dict)\n",
    "agent.actor_target.load_state_dict(actor_state_dict)\n",
    "\n",
    "agent.critic.load_state_dict(critic_state_dict)\n",
    "agent.critic_target.load_state_dict(critic_state_dict)\n",
    "\n",
    "states = env.reset(False)\n",
    "score = 0                \n",
    "\n",
    "while True:\n",
    "    actions = agent.act(states, False)                \n",
    "    next_states, rewards, dones = env.step(actions)            \n",
    "    states = next_states  \n",
    "    score += np.mean(rewards) \n",
    "    if np.any(dones):                                     \n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
