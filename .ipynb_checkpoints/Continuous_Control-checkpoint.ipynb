{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "## 1. Explore the Environment\n",
    "\n",
    "First part of the notebook introduces Unity Reacher enviroment and lets you explore its state and actions spaces\n",
    "\n",
    "### 1.1. Install the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Create Unity environment wrapper\n",
    "We need to create Environment wrapper which allows our agent to communicate with Unity environment in an easy way. Just simplifying interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "class EnvironmentWrapper():\n",
    "    def __init__(self, file_name):\n",
    "        self.env = UnityEnvironment(file_name=file_name, worker_id=1)\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        \n",
    "        self.env_info = self.env.reset(train_mode=False)[self.brain_name]\n",
    "        states = self.env_info.vector_observations\n",
    "        self.state_size = states.shape[1]\n",
    "        \n",
    "    def reset(self, train_mode=False):\n",
    "        self.env_info = self.env.reset(train_mode=train_mode)[self.brain_name]\n",
    "        state = self.env_info.vector_observations\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def step(self, actions):\n",
    "        self.env_info = self.env.step(actions)[self.brain_name]       \n",
    "        next_state = self.env_info.vector_observations   \n",
    "        reward = self.env_info.rewards\n",
    "        #reward = [x * 10.0 for x in reward]\n",
    "        done = self.env_info.local_done                \n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load single agent environemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = EnvironmentWrapper(file_name=\"reacher/reacher.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Explore the Environemnt\n",
    "\n",
    "Show environment state and action sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print (env.action_size)\n",
    "print (env.state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='reacher-m/reacher.exe', )\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.env.brain_names[0]\n",
    "brain = env.env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.30408478e+00 -1.00000000e+00\n",
      " -4.92529202e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.33014059e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.36999999172985554\n"
     ]
    }
   ],
   "source": [
    "env_info = env.env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train DDPG based Agent\n",
    "\n",
    "Now it's time to train agent to solve the environment! When training the environment. We implemented DDPG algorithm described in the paper: https://arxiv.org/abs/1509.02971 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Import PyTorch\n",
    "First we need to import PyTorch library so we can start building two networks:  Policy Network (Actor) to generate actions an Q-Network (Critic) to calculate Q value of generated action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. Setup Network Archtiectures\n",
    "For the Policy network (Actor) we ended up with simple three layers fully connected network with Relu as activation functions and Batch Normalization  For Q-Network (Critic) we come up with four layers fully connected network with Relu as activation functions.  Dropout regularization is also used to improve Network generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.state_size =  state_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.state_size, 192)\n",
    "        self.batch_norm1 =  torch.nn.BatchNorm1d(192)\n",
    "        self.fc2 = nn.Linear(192, 96)\n",
    "        self.batch_norm2 =  torch.nn.BatchNorm1d(96)\n",
    "        self.fc3 = nn.Linear(96, self.action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # Layer 1\n",
    "        x = self.fc1(state)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = F.tanh(self.fc3(x))     \n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Q-Network) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.state_size =  state_size\n",
    "        \n",
    "        self.fc1s = nn.Linear(self.state_size, 128)\n",
    "        self.fc2 = nn.Linear(128 + self.action_size, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "                \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # State feature layer\n",
    "        xs = F.relu(self.fc1s(state))\n",
    "        \n",
    "        # State freatures + action values\n",
    "        x =  torch.cat((xs, action), dim=1)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 4\n",
    "        x = self.fc4(x)     \n",
    "         \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Create Expirience Replay Buffer\n",
    "We need to define class with will be used to store experience tuples. DDPG algorithm also uses previous expirience similar to the DQN for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Create Ornstein-Uhlenbeck process class\n",
    "\n",
    "As was mentioned in the original paper a major challenge of learning in continuous action spaces is exploration. So to make agent to explore we are adding some noise to the actions using an Ornstein-Uhlenbeck process.  So we need to define noise class to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Create agent\n",
    "\n",
    "Create DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# hyper params sample\n",
    "hyperparams = { \"BUFFER_SIZE\" : int(1e5),  # replay buffer size\n",
    "                \"BATCH_SIZE\" : 64,         # minibatch size\n",
    "                \"GAMMA\" : 0.99,            # discount factor\n",
    "                \"TAU\" : 1e-3,              # for soft update of target parameters\n",
    "                \"LR\" : 1e-3,               # learning rate \n",
    "                \"LEARN_EVERY\" : 4,         # how often to update the networks\n",
    "                \"LEARN_ITERATIONS\" : 10,   # how many iterations needed for each network update\n",
    "              }\n",
    "\n",
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, device, hyperparams, actor, critic, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an DDPG Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.hyperparams = hyperparams\n",
    "        \n",
    "        self.device = device\n",
    "           \n",
    "        self.BUFFER_SIZE = hyperparams[\"BUFFER_SIZE\"]\n",
    "        self.BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "        self.GAMMA = hyperparams[\"GAMMA\"]\n",
    "        self.TAU = hyperparams[\"TAU\"]\n",
    "        self.LR = hyperparams[\"LR\"]\n",
    "        \n",
    "        self.LEARN_EVERY = hyperparams[\"LEARN_EVERY\"]\n",
    "        self.LEARN_ITERATIONS = hyperparams[\"LEARN_ITERATIONS\"]\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Actor\n",
    "        self.actor = actor(state_size, action_size, seed).to(self.device)\n",
    "        self.actor_target = actor(state_size, action_size, seed).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.LR)\n",
    "        self.actor.eval()\n",
    "       \n",
    "         # Critic\n",
    "        self.critic = critic(state_size, action_size, seed).to(self.device)\n",
    "        self.critic_target = critic(state_size, action_size, seed).to(self.device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.LR)\n",
    "        self.critic.train()\n",
    "        \n",
    "        # Deep Q-Learning Algorithm: Initialize replay memory D with N = BUFFER_SIZE\n",
    "        self.memory = ReplayBuffer(action_size, self.BUFFER_SIZE, self.BATCH_SIZE, seed)\n",
    "       \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "         # Noise process\n",
    "        self.noise = OUNoise(action_size, seed)\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience tuple (S, A, R, S~) in replay memory D\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.LEARN_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in the memory, run learning cycle\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                # run learning cycle LEARN_ITERATIONS times \n",
    "                for _ in range(self.LEARN_ITERATIONS):\n",
    "                    # Obtain random minibatch of tuples (S, A, R, S~) from replay memory D\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, self.GAMMA)\n",
    "                    \n",
    "                    # update target networks using soft update\n",
    "                    self.soft_update(self.actor, self.actor_target, self.TAU)   \n",
    "                    self.soft_update(self.critic, self.critic_target, self.TAU)   \n",
    "    \n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        \n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        self.actor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_values = self.actor(states).cpu().data.numpy()\n",
    "        self.actor.train()\n",
    "        \n",
    "        # add noise to the actions to favor exploartion\n",
    "        if add_noise:\n",
    "            action_values += self.noise.sample() / 10.0\n",
    "        \n",
    "        # make sure that resulting value after adding noise is still in [-1, 1] interval\n",
    "        return np.clip(action_values, -1.0, 1.0)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use random minibatch of tuples(S[j], A[j], R[j], S[j+1]) from replay memory D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        with torch.no_grad():            \n",
    "            # get continious actions vector according to the target policy network (Actor)\n",
    "            # for the next states\n",
    "            actions_next = self.actor_target(next_states)\n",
    "            \n",
    "            # evaluate q values for the actions proposed by policy network for the next states\n",
    "            # Q(states_next, actions_next) by target Q network (Critic)\n",
    "            Q_states_next_actions_next = self.critic_target(next_states,  actions_next)\n",
    "                       \n",
    "            # calculate target value\n",
    "            # y[j]= r[j] if episode terminates (done = 1)\n",
    "            y = rewards + gamma * Q_states_next_actions_next * (1.0 - dones)\n",
    "        \n",
    "        # get expected Q value \n",
    "        Q_expected = self.critic(states, actions)\n",
    "        \n",
    "        # critic loss is normal MSE loss, we need to minimize\n",
    "        # difference between actual Q values and target Q values for specific state, action pairs\n",
    "        critic_loss = F.mse_loss(Q_expected, y)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic.parameters(), 1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # predict new actions according to current policy network\n",
    "        predicted_actions = self.actor(states)\n",
    "        \n",
    "        # we need to maximize Q values calculated by critic based on predicted actions,\n",
    "        # so we need to perform gradient ascent (minus sign)\n",
    "        actor_loss = -self.critic(states, predicted_actions).mean()\n",
    "        \n",
    "        # backpropagation\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.noise.reset()\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Setup training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=100, max_t=1000, model_file_actor = 'actor.pth', model_file_critic = 'critic.pth'):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    all_scores = []                     # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)   # last 100 scores\n",
    "    scores_window10 = deque(maxlen=10)  # last 10 scores\n",
    "    max_score = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # Not sure why - sometimes unity environment is not reset after first call.\n",
    "        # so call reset two times - works like a charm\n",
    "        states = env.reset(train_mode=True)\n",
    "        states = env.reset(train_mode=True)\n",
    "        \n",
    "        # reset scores for the episode\n",
    "        scores = np.zeros(states.shape[0], dtype=float)\n",
    "        \n",
    "        # for time step t <- 1 to T:\n",
    "        for t in range(max_t):\n",
    "           \n",
    "            # Choose action from state S using policy network:\n",
    "            actions = agent.act(states)\n",
    "        \n",
    "            # Take an action and observe result\n",
    "            next_states, rewards, dones = env.step(actions)\n",
    "         \n",
    "            # Sample and Learn section\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break \n",
    "      \n",
    "        mean_score = np.mean(scores)\n",
    "        scores_window10.append(mean_score)\n",
    "        scores_window.append(mean_score)       # save most recent score\n",
    "        all_scores.append(mean_score)              # save most recent score\n",
    "        \n",
    "        print('Episode {}\\t Episode score: {:.3f} \\t Mean-10 score: {:.3f} \\t Mean-100 score: {:.3f}'\n",
    "              .format(i_episode, mean_score, np.mean(scores_window10), np.mean(scores_window)))\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score over 100 episodes: {:.3f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "            if np.mean(scores_window) > max_score:\n",
    "                print('Saving model... Episode: {} \\tAverage Score: {:.6f}\\n'.format(i_episode, np.mean(scores_window)))\n",
    "                torch.save(agent.actor.state_dict(), model_file_actor)\n",
    "                torch.save(agent.critic.state_dict(), model_file_critic)\n",
    "                max_score = np.mean(scores_window)\n",
    "        \n",
    "        if np.mean(scores_window) > 30.5:\n",
    "                print('Solved! Saving model... Episode: {} \\tAverage Score: {:.6f}\\n'.format(i_episode, np.mean(scores_window)))\n",
    "                torch.save(agent.actor.state_dict(), model_file_actor)\n",
    "                torch.save(agent.critic.state_dict(), model_file_critic)\n",
    "                max_score = np.mean(scores_window)\n",
    "                break\n",
    "                \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Set device and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "    \n",
    "def moving_average(data_set, periods=3):\n",
    "    weights = np.ones(periods) / periods\n",
    "    return np.convolve(data_set, weights, mode='valid')\n",
    "\n",
    "def plot_comparison(models,  labels, periods):\n",
    "    \n",
    "    fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    \n",
    "    for m, l in zip(models, labels):\n",
    "        moving_m =  moving_average(m, periods)\n",
    "        plt.plot(moving_m, label=l)\n",
    "    \n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.title(\"DQN Models comparison\", fontsize=12, fontweight='bold')\n",
    "    plt.xlabel(\"Eisode #\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\t Episode score: 0.750 \t Mean-10 score: 0.750 \t Mean-100 score: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\ipykernel_launcher.py:143: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\t Episode score: 0.220 \t Mean-10 score: 0.485 \t Mean-100 score: 0.485\n",
      "Episode 3\t Episode score: 0.270 \t Mean-10 score: 0.413 \t Mean-100 score: 0.413\n",
      "Episode 4\t Episode score: 1.170 \t Mean-10 score: 0.602 \t Mean-100 score: 0.602\n",
      "Episode 5\t Episode score: 1.200 \t Mean-10 score: 0.722 \t Mean-100 score: 0.722\n",
      "Episode 6\t Episode score: 0.410 \t Mean-10 score: 0.670 \t Mean-100 score: 0.670\n",
      "Episode 7\t Episode score: 0.230 \t Mean-10 score: 0.607 \t Mean-100 score: 0.607\n",
      "Episode 8\t Episode score: 0.000 \t Mean-10 score: 0.531 \t Mean-100 score: 0.531\n",
      "Episode 9\t Episode score: 3.030 \t Mean-10 score: 0.809 \t Mean-100 score: 0.809\n",
      "Episode 10\t Episode score: 0.480 \t Mean-10 score: 0.776 \t Mean-100 score: 0.776\n",
      "Episode 11\t Episode score: 0.520 \t Mean-10 score: 0.753 \t Mean-100 score: 0.753\n",
      "Episode 12\t Episode score: 0.430 \t Mean-10 score: 0.774 \t Mean-100 score: 0.726\n",
      "Episode 13\t Episode score: 0.270 \t Mean-10 score: 0.774 \t Mean-100 score: 0.691\n",
      "Episode 14\t Episode score: 0.000 \t Mean-10 score: 0.657 \t Mean-100 score: 0.641\n",
      "Episode 15\t Episode score: 0.700 \t Mean-10 score: 0.607 \t Mean-100 score: 0.645\n",
      "Episode 16\t Episode score: 1.280 \t Mean-10 score: 0.694 \t Mean-100 score: 0.685\n",
      "Episode 17\t Episode score: 2.600 \t Mean-10 score: 0.931 \t Mean-100 score: 0.798\n",
      "Episode 18\t Episode score: 2.020 \t Mean-10 score: 1.133 \t Mean-100 score: 0.866\n",
      "Episode 19\t Episode score: 0.540 \t Mean-10 score: 0.884 \t Mean-100 score: 0.848\n",
      "Episode 20\t Episode score: 1.730 \t Mean-10 score: 1.009 \t Mean-100 score: 0.892\n",
      "Episode 21\t Episode score: 2.320 \t Mean-10 score: 1.189 \t Mean-100 score: 0.960\n",
      "Episode 22\t Episode score: 1.930 \t Mean-10 score: 1.339 \t Mean-100 score: 1.005\n",
      "Episode 23\t Episode score: 0.770 \t Mean-10 score: 1.389 \t Mean-100 score: 0.994\n",
      "Episode 24\t Episode score: 3.560 \t Mean-10 score: 1.745 \t Mean-100 score: 1.101\n",
      "Episode 25\t Episode score: 2.920 \t Mean-10 score: 1.967 \t Mean-100 score: 1.174\n",
      "Episode 26\t Episode score: 1.150 \t Mean-10 score: 1.954 \t Mean-100 score: 1.173\n",
      "Episode 27\t Episode score: 2.730 \t Mean-10 score: 1.967 \t Mean-100 score: 1.231\n",
      "Episode 28\t Episode score: 2.140 \t Mean-10 score: 1.979 \t Mean-100 score: 1.263\n",
      "Episode 29\t Episode score: 5.520 \t Mean-10 score: 2.477 \t Mean-100 score: 1.410\n",
      "Episode 30\t Episode score: 3.400 \t Mean-10 score: 2.644 \t Mean-100 score: 1.476\n",
      "Episode 31\t Episode score: 3.880 \t Mean-10 score: 2.800 \t Mean-100 score: 1.554\n",
      "Episode 32\t Episode score: 3.600 \t Mean-10 score: 2.967 \t Mean-100 score: 1.618\n",
      "Episode 33\t Episode score: 2.920 \t Mean-10 score: 3.182 \t Mean-100 score: 1.657\n",
      "Episode 34\t Episode score: 4.670 \t Mean-10 score: 3.293 \t Mean-100 score: 1.746\n",
      "Episode 35\t Episode score: 4.360 \t Mean-10 score: 3.437 \t Mean-100 score: 1.821\n",
      "Episode 36\t Episode score: 3.510 \t Mean-10 score: 3.673 \t Mean-100 score: 1.867\n",
      "Episode 37\t Episode score: 2.870 \t Mean-10 score: 3.687 \t Mean-100 score: 1.895\n",
      "Episode 38\t Episode score: 2.570 \t Mean-10 score: 3.730 \t Mean-100 score: 1.912\n",
      "Episode 39\t Episode score: 4.370 \t Mean-10 score: 3.615 \t Mean-100 score: 1.975\n",
      "Episode 40\t Episode score: 4.770 \t Mean-10 score: 3.752 \t Mean-100 score: 2.045\n",
      "Episode 41\t Episode score: 4.760 \t Mean-10 score: 3.840 \t Mean-100 score: 2.111\n",
      "Episode 42\t Episode score: 7.480 \t Mean-10 score: 4.228 \t Mean-100 score: 2.239\n",
      "Episode 43\t Episode score: 3.400 \t Mean-10 score: 4.276 \t Mean-100 score: 2.266\n",
      "Episode 44\t Episode score: 5.410 \t Mean-10 score: 4.350 \t Mean-100 score: 2.338\n",
      "Episode 45\t Episode score: 5.530 \t Mean-10 score: 4.467 \t Mean-100 score: 2.409\n",
      "Episode 46\t Episode score: 3.980 \t Mean-10 score: 4.514 \t Mean-100 score: 2.443\n",
      "Episode 47\t Episode score: 6.250 \t Mean-10 score: 4.852 \t Mean-100 score: 2.524\n",
      "Episode 48\t Episode score: 6.330 \t Mean-10 score: 5.228 \t Mean-100 score: 2.603\n",
      "Episode 49\t Episode score: 4.910 \t Mean-10 score: 5.282 \t Mean-100 score: 2.650\n",
      "Episode 50\t Episode score: 8.450 \t Mean-10 score: 5.650 \t Mean-100 score: 2.766\n",
      "Episode 51\t Episode score: 6.560 \t Mean-10 score: 5.830 \t Mean-100 score: 2.841\n",
      "Episode 52\t Episode score: 6.620 \t Mean-10 score: 5.744 \t Mean-100 score: 2.913\n",
      "Episode 53\t Episode score: 6.420 \t Mean-10 score: 6.046 \t Mean-100 score: 2.979\n",
      "Episode 54\t Episode score: 7.190 \t Mean-10 score: 6.224 \t Mean-100 score: 3.057\n",
      "Episode 55\t Episode score: 2.610 \t Mean-10 score: 5.932 \t Mean-100 score: 3.049\n",
      "Episode 56\t Episode score: 4.650 \t Mean-10 score: 5.999 \t Mean-100 score: 3.078\n",
      "Episode 57\t Episode score: 4.690 \t Mean-10 score: 5.843 \t Mean-100 score: 3.106\n",
      "Episode 58\t Episode score: 9.300 \t Mean-10 score: 6.140 \t Mean-100 score: 3.213\n",
      "Episode 59\t Episode score: 6.740 \t Mean-10 score: 6.323 \t Mean-100 score: 3.273\n",
      "Episode 60\t Episode score: 9.160 \t Mean-10 score: 6.394 \t Mean-100 score: 3.371\n",
      "Episode 61\t Episode score: 8.190 \t Mean-10 score: 6.557 \t Mean-100 score: 3.450\n",
      "Episode 62\t Episode score: 4.760 \t Mean-10 score: 6.371 \t Mean-100 score: 3.471\n",
      "Episode 63\t Episode score: 7.580 \t Mean-10 score: 6.487 \t Mean-100 score: 3.536\n",
      "Episode 64\t Episode score: 6.860 \t Mean-10 score: 6.454 \t Mean-100 score: 3.588\n",
      "Episode 65\t Episode score: 8.610 \t Mean-10 score: 7.054 \t Mean-100 score: 3.665\n",
      "Episode 66\t Episode score: 6.240 \t Mean-10 score: 7.213 \t Mean-100 score: 3.704\n",
      "Episode 67\t Episode score: 8.220 \t Mean-10 score: 7.566 \t Mean-100 score: 3.772\n",
      "Episode 68\t Episode score: 9.150 \t Mean-10 score: 7.551 \t Mean-100 score: 3.851\n",
      "Episode 69\t Episode score: 9.970 \t Mean-10 score: 7.874 \t Mean-100 score: 3.940\n",
      "Episode 70\t Episode score: 7.220 \t Mean-10 score: 7.680 \t Mean-100 score: 3.986\n",
      "Episode 71\t Episode score: 3.230 \t Mean-10 score: 7.184 \t Mean-100 score: 3.976\n",
      "Episode 72\t Episode score: 12.170 \t Mean-10 score: 7.925 \t Mean-100 score: 4.090\n",
      "Episode 73\t Episode score: 9.450 \t Mean-10 score: 8.112 \t Mean-100 score: 4.163\n",
      "Episode 74\t Episode score: 8.850 \t Mean-10 score: 8.311 \t Mean-100 score: 4.226\n",
      "Episode 75\t Episode score: 6.340 \t Mean-10 score: 8.084 \t Mean-100 score: 4.255\n",
      "Episode 76\t Episode score: 7.420 \t Mean-10 score: 8.202 \t Mean-100 score: 4.296\n",
      "Episode 77\t Episode score: 8.830 \t Mean-10 score: 8.263 \t Mean-100 score: 4.355\n",
      "Episode 78\t Episode score: 11.370 \t Mean-10 score: 8.485 \t Mean-100 score: 4.445\n",
      "Episode 79\t Episode score: 7.170 \t Mean-10 score: 8.205 \t Mean-100 score: 4.479\n",
      "Episode 80\t Episode score: 9.480 \t Mean-10 score: 8.431 \t Mean-100 score: 4.542\n",
      "Episode 81\t Episode score: 11.860 \t Mean-10 score: 9.294 \t Mean-100 score: 4.632\n",
      "Episode 82\t Episode score: 11.520 \t Mean-10 score: 9.229 \t Mean-100 score: 4.716\n",
      "Episode 83\t Episode score: 7.970 \t Mean-10 score: 9.081 \t Mean-100 score: 4.756\n",
      "Episode 84\t Episode score: 11.830 \t Mean-10 score: 9.379 \t Mean-100 score: 4.840\n",
      "Episode 85\t Episode score: 13.140 \t Mean-10 score: 10.059 \t Mean-100 score: 4.937\n",
      "Episode 86\t Episode score: 16.040 \t Mean-10 score: 10.921 \t Mean-100 score: 5.067\n",
      "Episode 87\t Episode score: 14.340 \t Mean-10 score: 11.472 \t Mean-100 score: 5.173\n",
      "Episode 88\t Episode score: 9.770 \t Mean-10 score: 11.312 \t Mean-100 score: 5.225\n",
      "Episode 89\t Episode score: 10.780 \t Mean-10 score: 11.673 \t Mean-100 score: 5.288\n",
      "Episode 90\t Episode score: 6.750 \t Mean-10 score: 11.400 \t Mean-100 score: 5.304\n",
      "Episode 91\t Episode score: 11.070 \t Mean-10 score: 11.321 \t Mean-100 score: 5.367\n",
      "Episode 92\t Episode score: 8.340 \t Mean-10 score: 11.003 \t Mean-100 score: 5.400\n",
      "Episode 93\t Episode score: 9.880 \t Mean-10 score: 11.194 \t Mean-100 score: 5.448\n",
      "Episode 94\t Episode score: 10.740 \t Mean-10 score: 11.085 \t Mean-100 score: 5.504\n",
      "Episode 95\t Episode score: 13.700 \t Mean-10 score: 11.141 \t Mean-100 score: 5.590\n",
      "Episode 96\t Episode score: 11.480 \t Mean-10 score: 10.685 \t Mean-100 score: 5.652\n",
      "Episode 97\t Episode score: 11.710 \t Mean-10 score: 10.422 \t Mean-100 score: 5.714\n",
      "Episode 98\t Episode score: 12.140 \t Mean-10 score: 10.659 \t Mean-100 score: 5.780\n",
      "Episode 99\t Episode score: 8.000 \t Mean-10 score: 10.381 \t Mean-100 score: 5.802\n",
      "Episode 100\t Episode score: 9.200 \t Mean-10 score: 10.626 \t Mean-100 score: 5.836\n",
      "Episode 100\tAverage Score over 100 episodes: 5.836\n",
      "Saving model... Episode: 100 \tAverage Score: 5.836200\n",
      "\n",
      "Episode 101\t Episode score: 12.830 \t Mean-10 score: 10.802 \t Mean-100 score: 5.957\n",
      "Episode 102\t Episode score: 9.270 \t Mean-10 score: 10.895 \t Mean-100 score: 6.047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 103\t Episode score: 13.360 \t Mean-10 score: 11.243 \t Mean-100 score: 6.178\n",
      "Episode 104\t Episode score: 13.430 \t Mean-10 score: 11.512 \t Mean-100 score: 6.301\n",
      "Episode 105\t Episode score: 12.400 \t Mean-10 score: 11.382 \t Mean-100 score: 6.413\n",
      "Episode 106\t Episode score: 13.530 \t Mean-10 score: 11.587 \t Mean-100 score: 6.544\n",
      "Episode 107\t Episode score: 8.570 \t Mean-10 score: 11.273 \t Mean-100 score: 6.628\n",
      "Episode 108\t Episode score: 15.280 \t Mean-10 score: 11.587 \t Mean-100 score: 6.780\n",
      "Episode 109\t Episode score: 10.400 \t Mean-10 score: 11.827 \t Mean-100 score: 6.854\n",
      "Episode 110\t Episode score: 8.440 \t Mean-10 score: 11.751 \t Mean-100 score: 6.934\n",
      "Episode 111\t Episode score: 11.320 \t Mean-10 score: 11.600 \t Mean-100 score: 7.042\n",
      "Episode 112\t Episode score: 10.230 \t Mean-10 score: 11.696 \t Mean-100 score: 7.140\n",
      "Episode 113\t Episode score: 12.150 \t Mean-10 score: 11.575 \t Mean-100 score: 7.258\n",
      "Episode 114\t Episode score: 13.010 \t Mean-10 score: 11.533 \t Mean-100 score: 7.389\n",
      "Episode 115\t Episode score: 16.070 \t Mean-10 score: 11.900 \t Mean-100 score: 7.542\n",
      "Episode 116\t Episode score: 15.460 \t Mean-10 score: 12.093 \t Mean-100 score: 7.684\n",
      "Episode 117\t Episode score: 16.390 \t Mean-10 score: 12.875 \t Mean-100 score: 7.822\n",
      "Episode 118\t Episode score: 11.500 \t Mean-10 score: 12.497 \t Mean-100 score: 7.917\n",
      "Episode 119\t Episode score: 18.510 \t Mean-10 score: 13.308 \t Mean-100 score: 8.096\n",
      "Episode 120\t Episode score: 17.480 \t Mean-10 score: 14.212 \t Mean-100 score: 8.254\n",
      "Episode 121\t Episode score: 16.620 \t Mean-10 score: 14.742 \t Mean-100 score: 8.397\n",
      "Episode 122\t Episode score: 14.270 \t Mean-10 score: 15.146 \t Mean-100 score: 8.520\n",
      "Episode 123\t Episode score: 20.900 \t Mean-10 score: 16.021 \t Mean-100 score: 8.722\n",
      "Episode 124\t Episode score: 9.950 \t Mean-10 score: 15.715 \t Mean-100 score: 8.786\n",
      "Episode 125\t Episode score: 18.510 \t Mean-10 score: 15.959 \t Mean-100 score: 8.941\n",
      "Episode 126\t Episode score: 14.250 \t Mean-10 score: 15.838 \t Mean-100 score: 9.072\n",
      "Episode 127\t Episode score: 9.290 \t Mean-10 score: 15.128 \t Mean-100 score: 9.138\n",
      "Episode 128\t Episode score: 25.090 \t Mean-10 score: 16.487 \t Mean-100 score: 9.368\n",
      "Episode 129\t Episode score: 15.010 \t Mean-10 score: 16.137 \t Mean-100 score: 9.462\n",
      "Episode 130\t Episode score: 17.220 \t Mean-10 score: 16.111 \t Mean-100 score: 9.601\n",
      "Episode 131\t Episode score: 12.910 \t Mean-10 score: 15.740 \t Mean-100 score: 9.691\n",
      "Episode 132\t Episode score: 21.620 \t Mean-10 score: 16.475 \t Mean-100 score: 9.871\n",
      "Episode 133\t Episode score: 12.480 \t Mean-10 score: 15.633 \t Mean-100 score: 9.967\n",
      "Episode 134\t Episode score: 19.680 \t Mean-10 score: 16.606 \t Mean-100 score: 10.117\n",
      "Episode 135\t Episode score: 18.910 \t Mean-10 score: 16.646 \t Mean-100 score: 10.262\n",
      "Episode 136\t Episode score: 17.820 \t Mean-10 score: 17.003 \t Mean-100 score: 10.405\n",
      "Episode 137\t Episode score: 19.390 \t Mean-10 score: 18.013 \t Mean-100 score: 10.571\n",
      "Episode 138\t Episode score: 18.220 \t Mean-10 score: 17.326 \t Mean-100 score: 10.727\n",
      "Episode 139\t Episode score: 14.640 \t Mean-10 score: 17.289 \t Mean-100 score: 10.830\n",
      "Episode 140\t Episode score: 22.320 \t Mean-10 score: 17.799 \t Mean-100 score: 11.005\n",
      "Episode 141\t Episode score: 12.660 \t Mean-10 score: 17.774 \t Mean-100 score: 11.084\n",
      "Episode 142\t Episode score: 16.080 \t Mean-10 score: 17.220 \t Mean-100 score: 11.170\n",
      "Episode 143\t Episode score: 24.420 \t Mean-10 score: 18.414 \t Mean-100 score: 11.381\n",
      "Episode 144\t Episode score: 18.640 \t Mean-10 score: 18.310 \t Mean-100 score: 11.513\n",
      "Episode 145\t Episode score: 18.190 \t Mean-10 score: 18.238 \t Mean-100 score: 11.639\n",
      "Episode 146\t Episode score: 14.940 \t Mean-10 score: 17.950 \t Mean-100 score: 11.749\n",
      "Episode 147\t Episode score: 14.990 \t Mean-10 score: 17.510 \t Mean-100 score: 11.836\n",
      "Episode 148\t Episode score: 20.740 \t Mean-10 score: 17.762 \t Mean-100 score: 11.981\n",
      "Episode 149\t Episode score: 16.520 \t Mean-10 score: 17.950 \t Mean-100 score: 12.097\n",
      "Episode 150\t Episode score: 17.930 \t Mean-10 score: 17.511 \t Mean-100 score: 12.191\n",
      "Episode 151\t Episode score: 11.450 \t Mean-10 score: 17.390 \t Mean-100 score: 12.240\n",
      "Episode 152\t Episode score: 13.510 \t Mean-10 score: 17.133 \t Mean-100 score: 12.309\n",
      "Episode 153\t Episode score: 20.600 \t Mean-10 score: 16.751 \t Mean-100 score: 12.451\n",
      "Episode 154\t Episode score: 14.570 \t Mean-10 score: 16.344 \t Mean-100 score: 12.525\n",
      "Episode 155\t Episode score: 13.860 \t Mean-10 score: 15.911 \t Mean-100 score: 12.637\n",
      "Episode 156\t Episode score: 18.540 \t Mean-10 score: 16.271 \t Mean-100 score: 12.776\n",
      "Episode 157\t Episode score: 19.610 \t Mean-10 score: 16.733 \t Mean-100 score: 12.925\n",
      "Episode 158\t Episode score: 18.140 \t Mean-10 score: 16.473 \t Mean-100 score: 13.014\n",
      "Episode 159\t Episode score: 15.810 \t Mean-10 score: 16.402 \t Mean-100 score: 13.105\n",
      "Episode 160\t Episode score: 15.280 \t Mean-10 score: 16.137 \t Mean-100 score: 13.166\n",
      "Episode 161\t Episode score: 26.420 \t Mean-10 score: 17.634 \t Mean-100 score: 13.348\n",
      "Episode 162\t Episode score: 16.810 \t Mean-10 score: 17.964 \t Mean-100 score: 13.469\n",
      "Episode 163\t Episode score: 21.810 \t Mean-10 score: 18.085 \t Mean-100 score: 13.611\n",
      "Episode 164\t Episode score: 22.110 \t Mean-10 score: 18.839 \t Mean-100 score: 13.763\n",
      "Episode 165\t Episode score: 20.770 \t Mean-10 score: 19.530 \t Mean-100 score: 13.885\n",
      "Episode 166\t Episode score: 18.500 \t Mean-10 score: 19.526 \t Mean-100 score: 14.008\n",
      "Episode 167\t Episode score: 25.960 \t Mean-10 score: 20.161 \t Mean-100 score: 14.185\n",
      "Episode 168\t Episode score: 15.520 \t Mean-10 score: 19.899 \t Mean-100 score: 14.249\n",
      "Episode 169\t Episode score: 29.260 \t Mean-10 score: 21.244 \t Mean-100 score: 14.442\n",
      "Episode 170\t Episode score: 19.770 \t Mean-10 score: 21.693 \t Mean-100 score: 14.567\n",
      "Episode 171\t Episode score: 14.270 \t Mean-10 score: 20.478 \t Mean-100 score: 14.677\n",
      "Episode 172\t Episode score: 21.730 \t Mean-10 score: 20.970 \t Mean-100 score: 14.773\n",
      "Episode 173\t Episode score: 18.600 \t Mean-10 score: 20.649 \t Mean-100 score: 14.865\n",
      "Episode 174\t Episode score: 18.650 \t Mean-10 score: 20.303 \t Mean-100 score: 14.963\n",
      "Episode 175\t Episode score: 21.510 \t Mean-10 score: 20.377 \t Mean-100 score: 15.114\n",
      "Episode 176\t Episode score: 20.980 \t Mean-10 score: 20.625 \t Mean-100 score: 15.250\n",
      "Episode 177\t Episode score: 23.800 \t Mean-10 score: 20.409 \t Mean-100 score: 15.400\n",
      "Episode 178\t Episode score: 32.690 \t Mean-10 score: 22.126 \t Mean-100 score: 15.613\n",
      "Episode 179\t Episode score: 14.230 \t Mean-10 score: 20.623 \t Mean-100 score: 15.683\n",
      "Episode 180\t Episode score: 25.180 \t Mean-10 score: 21.164 \t Mean-100 score: 15.840\n",
      "Episode 181\t Episode score: 18.130 \t Mean-10 score: 21.550 \t Mean-100 score: 15.903\n",
      "Episode 182\t Episode score: 19.130 \t Mean-10 score: 21.290 \t Mean-100 score: 15.979\n",
      "Episode 183\t Episode score: 19.510 \t Mean-10 score: 21.381 \t Mean-100 score: 16.095\n",
      "Episode 184\t Episode score: 26.220 \t Mean-10 score: 22.138 \t Mean-100 score: 16.238\n",
      "Episode 185\t Episode score: 22.600 \t Mean-10 score: 22.247 \t Mean-100 score: 16.333\n",
      "Episode 186\t Episode score: 17.190 \t Mean-10 score: 21.868 \t Mean-100 score: 16.345\n",
      "Episode 187\t Episode score: 23.330 \t Mean-10 score: 21.821 \t Mean-100 score: 16.434\n",
      "Episode 188\t Episode score: 28.400 \t Mean-10 score: 21.392 \t Mean-100 score: 16.621\n",
      "Episode 189\t Episode score: 24.930 \t Mean-10 score: 22.462 \t Mean-100 score: 16.762\n",
      "Episode 190\t Episode score: 22.300 \t Mean-10 score: 22.174 \t Mean-100 score: 16.918\n",
      "Episode 191\t Episode score: 17.940 \t Mean-10 score: 22.155 \t Mean-100 score: 16.986\n",
      "Episode 192\t Episode score: 23.130 \t Mean-10 score: 22.555 \t Mean-100 score: 17.134\n",
      "Episode 193\t Episode score: 24.720 \t Mean-10 score: 23.076 \t Mean-100 score: 17.283\n",
      "Episode 194\t Episode score: 19.900 \t Mean-10 score: 22.444 \t Mean-100 score: 17.374\n",
      "Episode 195\t Episode score: 26.360 \t Mean-10 score: 22.820 \t Mean-100 score: 17.501\n",
      "Episode 196\t Episode score: 22.440 \t Mean-10 score: 23.345 \t Mean-100 score: 17.611\n",
      "Episode 197\t Episode score: 21.460 \t Mean-10 score: 23.158 \t Mean-100 score: 17.708\n",
      "Episode 198\t Episode score: 27.480 \t Mean-10 score: 23.066 \t Mean-100 score: 17.861\n",
      "Episode 199\t Episode score: 31.510 \t Mean-10 score: 23.724 \t Mean-100 score: 18.097\n",
      "Episode 200\t Episode score: 28.820 \t Mean-10 score: 24.376 \t Mean-100 score: 18.293\n",
      "Episode 200\tAverage Score over 100 episodes: 18.293\n",
      "Saving model... Episode: 200 \tAverage Score: 18.292800\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 201\t Episode score: 26.820 \t Mean-10 score: 25.264 \t Mean-100 score: 18.433\n",
      "Episode 202\t Episode score: 22.320 \t Mean-10 score: 25.183 \t Mean-100 score: 18.563\n",
      "Episode 203\t Episode score: 28.600 \t Mean-10 score: 25.571 \t Mean-100 score: 18.716\n",
      "Episode 204\t Episode score: 25.450 \t Mean-10 score: 26.126 \t Mean-100 score: 18.836\n",
      "Episode 205\t Episode score: 23.950 \t Mean-10 score: 25.885 \t Mean-100 score: 18.951\n",
      "Episode 206\t Episode score: 25.140 \t Mean-10 score: 26.155 \t Mean-100 score: 19.067\n",
      "Episode 207\t Episode score: 26.130 \t Mean-10 score: 26.622 \t Mean-100 score: 19.243\n",
      "Episode 208\t Episode score: 30.490 \t Mean-10 score: 26.923 \t Mean-100 score: 19.395\n",
      "Episode 209\t Episode score: 23.550 \t Mean-10 score: 26.127 \t Mean-100 score: 19.527\n",
      "Episode 210\t Episode score: 21.210 \t Mean-10 score: 25.366 \t Mean-100 score: 19.654\n",
      "Episode 211\t Episode score: 27.880 \t Mean-10 score: 25.472 \t Mean-100 score: 19.820\n",
      "Episode 212\t Episode score: 26.670 \t Mean-10 score: 25.907 \t Mean-100 score: 19.984\n",
      "Episode 213\t Episode score: 28.090 \t Mean-10 score: 25.856 \t Mean-100 score: 20.144\n",
      "Episode 214\t Episode score: 19.510 \t Mean-10 score: 25.262 \t Mean-100 score: 20.209\n",
      "Episode 215\t Episode score: 28.930 \t Mean-10 score: 25.760 \t Mean-100 score: 20.337\n",
      "Episode 216\t Episode score: 14.390 \t Mean-10 score: 24.685 \t Mean-100 score: 20.327\n",
      "Episode 217\t Episode score: 17.450 \t Mean-10 score: 23.817 \t Mean-100 score: 20.337\n",
      "Episode 218\t Episode score: 22.980 \t Mean-10 score: 23.066 \t Mean-100 score: 20.452\n",
      "Episode 219\t Episode score: 20.200 \t Mean-10 score: 22.731 \t Mean-100 score: 20.469\n",
      "Episode 220\t Episode score: 21.330 \t Mean-10 score: 22.743 \t Mean-100 score: 20.507\n",
      "Episode 221\t Episode score: 26.780 \t Mean-10 score: 22.633 \t Mean-100 score: 20.609\n",
      "Episode 222\t Episode score: 24.690 \t Mean-10 score: 22.435 \t Mean-100 score: 20.713\n",
      "Episode 223\t Episode score: 28.450 \t Mean-10 score: 22.471 \t Mean-100 score: 20.789\n",
      "Episode 224\t Episode score: 23.080 \t Mean-10 score: 22.828 \t Mean-100 score: 20.920\n",
      "Episode 225\t Episode score: 24.930 \t Mean-10 score: 22.428 \t Mean-100 score: 20.984\n",
      "Episode 226\t Episode score: 32.920 \t Mean-10 score: 24.281 \t Mean-100 score: 21.171\n",
      "Episode 227\t Episode score: 24.400 \t Mean-10 score: 24.976 \t Mean-100 score: 21.322\n",
      "Episode 228\t Episode score: 26.650 \t Mean-10 score: 25.343 \t Mean-100 score: 21.338\n",
      "Episode 229\t Episode score: 26.720 \t Mean-10 score: 25.995 \t Mean-100 score: 21.455\n",
      "Episode 230\t Episode score: 24.360 \t Mean-10 score: 26.298 \t Mean-100 score: 21.526\n",
      "Episode 231\t Episode score: 25.400 \t Mean-10 score: 26.160 \t Mean-100 score: 21.651\n",
      "Episode 232\t Episode score: 26.190 \t Mean-10 score: 26.310 \t Mean-100 score: 21.697\n",
      "Episode 233\t Episode score: 23.230 \t Mean-10 score: 25.788 \t Mean-100 score: 21.804\n",
      "Episode 234\t Episode score: 29.810 \t Mean-10 score: 26.461 \t Mean-100 score: 21.905\n",
      "Episode 235\t Episode score: 25.090 \t Mean-10 score: 26.477 \t Mean-100 score: 21.967\n",
      "Episode 236\t Episode score: 29.000 \t Mean-10 score: 26.085 \t Mean-100 score: 22.079\n",
      "Episode 237\t Episode score: 28.730 \t Mean-10 score: 26.518 \t Mean-100 score: 22.172\n",
      "Episode 238\t Episode score: 33.090 \t Mean-10 score: 27.162 \t Mean-100 score: 22.321\n",
      "Episode 239\t Episode score: 20.170 \t Mean-10 score: 26.507 \t Mean-100 score: 22.376\n",
      "Episode 240\t Episode score: 27.510 \t Mean-10 score: 26.822 \t Mean-100 score: 22.428\n",
      "Episode 241\t Episode score: 25.190 \t Mean-10 score: 26.801 \t Mean-100 score: 22.554\n",
      "Episode 242\t Episode score: 30.390 \t Mean-10 score: 27.221 \t Mean-100 score: 22.697\n",
      "Episode 243\t Episode score: 25.090 \t Mean-10 score: 27.407 \t Mean-100 score: 22.703\n",
      "Episode 244\t Episode score: 22.650 \t Mean-10 score: 26.691 \t Mean-100 score: 22.744\n",
      "Episode 245\t Episode score: 25.640 \t Mean-10 score: 26.746 \t Mean-100 score: 22.818\n",
      "Episode 246\t Episode score: 23.410 \t Mean-10 score: 26.187 \t Mean-100 score: 22.903\n",
      "Episode 247\t Episode score: 25.940 \t Mean-10 score: 25.908 \t Mean-100 score: 23.012\n",
      "Episode 248\t Episode score: 32.010 \t Mean-10 score: 25.800 \t Mean-100 score: 23.125\n",
      "Episode 249\t Episode score: 27.730 \t Mean-10 score: 26.556 \t Mean-100 score: 23.237\n",
      "Episode 250\t Episode score: 26.680 \t Mean-10 score: 26.473 \t Mean-100 score: 23.325\n",
      "Episode 251\t Episode score: 24.990 \t Mean-10 score: 26.453 \t Mean-100 score: 23.460\n",
      "Episode 252\t Episode score: 26.900 \t Mean-10 score: 26.104 \t Mean-100 score: 23.594\n",
      "Episode 253\t Episode score: 25.650 \t Mean-10 score: 26.160 \t Mean-100 score: 23.644\n",
      "Episode 254\t Episode score: 31.740 \t Mean-10 score: 27.069 \t Mean-100 score: 23.816\n",
      "Episode 255\t Episode score: 25.590 \t Mean-10 score: 27.064 \t Mean-100 score: 23.933\n",
      "Episode 256\t Episode score: 27.030 \t Mean-10 score: 27.426 \t Mean-100 score: 24.018\n",
      "Episode 257\t Episode score: 28.360 \t Mean-10 score: 27.668 \t Mean-100 score: 24.106\n",
      "Episode 258\t Episode score: 28.150 \t Mean-10 score: 27.282 \t Mean-100 score: 24.206\n",
      "Episode 259\t Episode score: 35.870 \t Mean-10 score: 28.096 \t Mean-100 score: 24.406\n",
      "Episode 260\t Episode score: 26.960 \t Mean-10 score: 28.124 \t Mean-100 score: 24.523\n",
      "Episode 261\t Episode score: 32.300 \t Mean-10 score: 28.855 \t Mean-100 score: 24.582\n",
      "Episode 262\t Episode score: 31.360 \t Mean-10 score: 29.301 \t Mean-100 score: 24.728\n",
      "Episode 263\t Episode score: 31.370 \t Mean-10 score: 29.873 \t Mean-100 score: 24.823\n",
      "Episode 264\t Episode score: 22.410 \t Mean-10 score: 28.940 \t Mean-100 score: 24.826\n",
      "Episode 265\t Episode score: 29.670 \t Mean-10 score: 29.348 \t Mean-100 score: 24.915\n",
      "Episode 266\t Episode score: 26.500 \t Mean-10 score: 29.295 \t Mean-100 score: 24.995\n",
      "Episode 267\t Episode score: 30.400 \t Mean-10 score: 29.499 \t Mean-100 score: 25.040\n",
      "Episode 268\t Episode score: 31.300 \t Mean-10 score: 29.814 \t Mean-100 score: 25.197\n",
      "Episode 269\t Episode score: 31.420 \t Mean-10 score: 29.369 \t Mean-100 score: 25.219\n",
      "Episode 270\t Episode score: 30.650 \t Mean-10 score: 29.738 \t Mean-100 score: 25.328\n",
      "Episode 271\t Episode score: 26.380 \t Mean-10 score: 29.146 \t Mean-100 score: 25.449\n",
      "Episode 272\t Episode score: 30.030 \t Mean-10 score: 29.013 \t Mean-100 score: 25.532\n",
      "Episode 273\t Episode score: 28.440 \t Mean-10 score: 28.720 \t Mean-100 score: 25.630\n",
      "Episode 274\t Episode score: 28.620 \t Mean-10 score: 29.341 \t Mean-100 score: 25.730\n",
      "Episode 275\t Episode score: 26.270 \t Mean-10 score: 29.001 \t Mean-100 score: 25.778\n",
      "Episode 276\t Episode score: 18.190 \t Mean-10 score: 28.170 \t Mean-100 score: 25.750\n",
      "Episode 277\t Episode score: 28.000 \t Mean-10 score: 27.930 \t Mean-100 score: 25.792\n",
      "Episode 278\t Episode score: 32.950 \t Mean-10 score: 28.095 \t Mean-100 score: 25.794\n",
      "Episode 279\t Episode score: 33.020 \t Mean-10 score: 28.255 \t Mean-100 score: 25.982\n",
      "Episode 280\t Episode score: 30.150 \t Mean-10 score: 28.205 \t Mean-100 score: 26.032\n",
      "Episode 281\t Episode score: 25.630 \t Mean-10 score: 28.130 \t Mean-100 score: 26.107\n",
      "Episode 282\t Episode score: 32.250 \t Mean-10 score: 28.352 \t Mean-100 score: 26.238\n",
      "Episode 283\t Episode score: 28.620 \t Mean-10 score: 28.370 \t Mean-100 score: 26.329\n",
      "Episode 284\t Episode score: 22.540 \t Mean-10 score: 27.762 \t Mean-100 score: 26.292\n",
      "Episode 285\t Episode score: 26.310 \t Mean-10 score: 27.766 \t Mean-100 score: 26.329\n",
      "Episode 286\t Episode score: 27.460 \t Mean-10 score: 28.693 \t Mean-100 score: 26.432\n",
      "Episode 287\t Episode score: 29.380 \t Mean-10 score: 28.831 \t Mean-100 score: 26.493\n",
      "Episode 288\t Episode score: 32.190 \t Mean-10 score: 28.755 \t Mean-100 score: 26.531\n",
      "Episode 289\t Episode score: 27.540 \t Mean-10 score: 28.207 \t Mean-100 score: 26.557\n",
      "Episode 290\t Episode score: 26.320 \t Mean-10 score: 27.824 \t Mean-100 score: 26.597\n",
      "Episode 291\t Episode score: 26.720 \t Mean-10 score: 27.933 \t Mean-100 score: 26.685\n",
      "Episode 292\t Episode score: 25.630 \t Mean-10 score: 27.271 \t Mean-100 score: 26.710\n",
      "Episode 293\t Episode score: 32.940 \t Mean-10 score: 27.703 \t Mean-100 score: 26.792\n",
      "Episode 294\t Episode score: 27.440 \t Mean-10 score: 28.193 \t Mean-100 score: 26.867\n",
      "Episode 295\t Episode score: 32.720 \t Mean-10 score: 28.834 \t Mean-100 score: 26.931\n",
      "Episode 296\t Episode score: 28.770 \t Mean-10 score: 28.965 \t Mean-100 score: 26.994\n",
      "Episode 297\t Episode score: 26.960 \t Mean-10 score: 28.723 \t Mean-100 score: 27.049\n",
      "Episode 298\t Episode score: 26.570 \t Mean-10 score: 28.161 \t Mean-100 score: 27.040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 299\t Episode score: 27.670 \t Mean-10 score: 28.174 \t Mean-100 score: 27.002\n",
      "Episode 300\t Episode score: 31.280 \t Mean-10 score: 28.670 \t Mean-100 score: 27.026\n",
      "Episode 300\tAverage Score over 100 episodes: 27.026\n",
      "Saving model... Episode: 300 \tAverage Score: 27.026299\n",
      "\n",
      "Episode 301\t Episode score: 33.360 \t Mean-10 score: 29.334 \t Mean-100 score: 27.092\n",
      "Episode 302\t Episode score: 27.880 \t Mean-10 score: 29.559 \t Mean-100 score: 27.147\n",
      "Episode 303\t Episode score: 31.980 \t Mean-10 score: 29.463 \t Mean-100 score: 27.181\n",
      "Episode 304\t Episode score: 28.910 \t Mean-10 score: 29.610 \t Mean-100 score: 27.216\n",
      "Episode 305\t Episode score: 33.280 \t Mean-10 score: 29.666 \t Mean-100 score: 27.309\n",
      "Episode 306\t Episode score: 31.630 \t Mean-10 score: 29.952 \t Mean-100 score: 27.374\n",
      "Episode 307\t Episode score: 29.590 \t Mean-10 score: 30.215 \t Mean-100 score: 27.408\n",
      "Episode 308\t Episode score: 30.060 \t Mean-10 score: 30.564 \t Mean-100 score: 27.404\n",
      "Episode 309\t Episode score: 30.610 \t Mean-10 score: 30.858 \t Mean-100 score: 27.475\n",
      "Episode 310\t Episode score: 31.940 \t Mean-10 score: 30.924 \t Mean-100 score: 27.582\n",
      "Episode 311\t Episode score: 30.750 \t Mean-10 score: 30.663 \t Mean-100 score: 27.611\n",
      "Episode 312\t Episode score: 28.850 \t Mean-10 score: 30.760 \t Mean-100 score: 27.633\n",
      "Episode 313\t Episode score: 32.250 \t Mean-10 score: 30.787 \t Mean-100 score: 27.674\n",
      "Episode 314\t Episode score: 28.290 \t Mean-10 score: 30.725 \t Mean-100 score: 27.762\n",
      "Episode 315\t Episode score: 33.190 \t Mean-10 score: 30.716 \t Mean-100 score: 27.805\n",
      "Episode 316\t Episode score: 28.790 \t Mean-10 score: 30.432 \t Mean-100 score: 27.949\n",
      "Episode 317\t Episode score: 30.680 \t Mean-10 score: 30.541 \t Mean-100 score: 28.081\n",
      "Episode 318\t Episode score: 29.580 \t Mean-10 score: 30.493 \t Mean-100 score: 28.147\n",
      "Episode 319\t Episode score: 28.890 \t Mean-10 score: 30.321 \t Mean-100 score: 28.234\n",
      "Episode 320\t Episode score: 30.230 \t Mean-10 score: 30.150 \t Mean-100 score: 28.323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-43d0c34e6c64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m222\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file_actor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DDPG_actor.pth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file_critic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DDPG_critic.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DDPGScores.npy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-3f7d194f7b59>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, n_episodes, max_t, model_file_actor, model_file_critic)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# Sample and Learn section\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-edb9be299fca>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARN_ITERATIONS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                     \u001b[1;31m# Obtain random minibatch of tuples (S, A, R, S~) from replay memory D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                     \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-4a32facb3aec>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \"\"\"\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alexey\\Anaconda3\\envs\\drlnd\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\Alexey\\Anaconda3\\envs\\drlnd\\lib\\asyncio\\selector_events.py\", line 141, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n"
     ]
    }
   ],
   "source": [
    "hyperparams = { \"BUFFER_SIZE\" : int(1e6),  # replay buffer size\n",
    "                \"BATCH_SIZE\" : 1024,         # minibatch size\n",
    "                \"GAMMA\" : 0.99,             # discount factor\n",
    "                \"TAU\" : 1e-3,               # for soft update of target parameters\n",
    "                \"LR\" : 1e-4,                # learning rate \n",
    "                \"LEARN_EVERY\" : 10,         # how often to update the network\n",
    "                \"LEARN_ITERATIONS\" : 10,    # how many iterations needed for each network update\n",
    "              }\n",
    "\n",
    "agent = DDPGAgent(device, hyperparams, Actor, Critic, state_size=env.state_size, action_size=env.action_size, seed=222)\n",
    "\n",
    "scores = train(env, agent, n_episodes=500, max_t=1000, model_file_actor = 'DDPG_actor.pth', model_file_critic = 'DDPG_critic.pth')\n",
    "\n",
    "np.save(\"DDPGScores.npy\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "aver = moving_average(scores, 10)\n",
    "\n",
    "plot_scores(aver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Load agent and watch performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6399999856948853\n"
     ]
    }
   ],
   "source": [
    "hyperparams = { \"BUFFER_SIZE\" : int(1e6),  # replay buffer size\n",
    "                \"BATCH_SIZE\" : 1024,         # minibatch size\n",
    "                \"GAMMA\" : 0.99,             # discount factor\n",
    "                \"TAU\" : 1e-3,               # for soft update of target parameters\n",
    "                \"LR\" : 1e-4,                # learning rate \n",
    "                \"LEARN_EVERY\" : 10,         # how often to update the network\n",
    "                \"LEARN_ITERATIONS\" : 10,    # how many iterations needed for each network update\n",
    "              }\n",
    "\n",
    "agent = DDPGAgent(device, hyperparams, Actor, Critic, state_size=env.state_size, action_size=env.action_size, seed=0)\n",
    "\n",
    "actor_state_dict = torch.load('DDPG_actor.pth')\n",
    "critic_state_dict = torch.load('DDPG_critic.pth')\n",
    "\n",
    "agent.actor.load_state_dict(actor_state_dict)\n",
    "agent.actor_target.load_state_dict(actor_state_dict)\n",
    "\n",
    "agent.critic.load_state_dict(critic_state_dict)\n",
    "agent.critic_target.load_state_dict(critic_state_dict)\n",
    "\n",
    "states = env.reset(False)\n",
    "score = 0                \n",
    "\n",
    "while True:\n",
    "    actions = agent.act(states, False)                \n",
    "    next_states, rewards, dones = env.step(actions)            \n",
    "    states = next_states  \n",
    "    score += np.mean(rewards) \n",
    "    if np.any(dones):                                     \n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
