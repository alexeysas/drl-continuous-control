{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "## 1. Explore the Environment\n",
    "\n",
    "First part of the notebook introduces Unity Reacher enviroment and lets you explore its state and actions spaces\n",
    "\n",
    "### 1.1. Install the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Create Unity environment wrapper\n",
    "We need to create Environment wrapper which allows our agent to communicate with Unity environment in an easy way. Just simplifying interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "class EnvironmentWrapper():\n",
    "    def __init__(self, file_name):\n",
    "        self.env = UnityEnvironment(file_name=file_name, worker_id=1)\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        \n",
    "        self.env_info = self.env.reset(train_mode=False)[self.brain_name]\n",
    "        states = self.env_info.vector_observations\n",
    "        self.state_size = states.shape[1]\n",
    "        \n",
    "    def reset(self, train_mode=False):\n",
    "        self.env_info = self.env.reset(train_mode=train_mode)[self.brain_name]\n",
    "        state = self.env_info.vector_observations\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def step(self, actions):\n",
    "        self.env_info = self.env.step(actions)[self.brain_name]       \n",
    "        next_state = self.env_info.vector_observations   \n",
    "        reward = self.env_info.rewards\n",
    "        #reward = [x * 10.0 for x in reward]\n",
    "        done = self.env_info.local_done                \n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load single agent environemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = EnvironmentWrapper(file_name=\"reacher/reacher.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Explore the Environemnt\n",
    "\n",
    "Show environment state and action sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print (env.action_size)\n",
    "print (env.state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='reacher-m/reacher.exe', )\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.env.brain_names[0]\n",
    "brain = env.env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.30408478e+00 -1.00000000e+00\n",
      " -4.92529202e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.33014059e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.36999999172985554\n"
     ]
    }
   ],
   "source": [
    "env_info = env.env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train DDPG based Agent\n",
    "\n",
    "Now it's time to train agent to solve the environment! When training the environment. We implemented DDPG algorithm described in the paper: https://arxiv.org/abs/1509.02971 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Import PyTorch\n",
    "First we need to import PyTorch library so we can start building two networks:  Policy Network (Actor) to generate actions an Q-Network (Critic) to calculate Q value of generated action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. Setup Network Archtiectures\n",
    "For the Policy network (Actor) we ended up with simple three layers fully connected network with Relu as activation functions and Batch Normalization  For Q-Network (Critic) we come up with four layers fully connected network with Relu as activation functions.  Dropout regularization is also used to improve Network generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.state_size =  state_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.state_size, 128)\n",
    "        self.batch_norm1 =  torch.nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.batch_norm2 =  torch.nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, self.action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # Layer 1\n",
    "        x = self.fc1(state)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = F.tanh(self.fc3(x))     \n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Q-Network) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.state_size =  state_size\n",
    "        \n",
    "        self.fc1s = nn.Linear(self.state_size, 128)\n",
    "        self.fc2 = nn.Linear(128 + self.action_size, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "                \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # State feature layer\n",
    "        xs = F.relu(self.fc1s(state))\n",
    "        \n",
    "        # State freatures + action values\n",
    "        x =  torch.cat((xs, action), dim=1)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Layer 4\n",
    "        x = self.fc4(x)     \n",
    "         \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Create Expirience Replay Buffer\n",
    "We need to define class with will be used to store experience tuples. DDPG algorithm also uses previous expirience similar to the DQN for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Create Ornstein-Uhlenbeck process class\n",
    "\n",
    "As was mentioned in the original paper a major challenge of learning in continuous action spaces is exploration. So to make agent to explore we are adding some noise to the actions using an Ornstein-Uhlenbeck process.  So we need to define noise class to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Create agent\n",
    "\n",
    "Create DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# hyper params sample\n",
    "hyperparams = { \"BUFFER_SIZE\" : int(1e5),  # replay buffer size\n",
    "                \"BATCH_SIZE\" : 64,         # minibatch size\n",
    "                \"GAMMA\" : 0.99,            # discount factor\n",
    "                \"TAU\" : 1e-3,              # for soft update of target parameters\n",
    "                \"LR\" : 1e-4,               # learning rate \n",
    "                \"LEARN_EVERY\" : 4,         # how often to update the networks\n",
    "                \"LEARN_ITERATIONS\" : 10,   # how many iterations needed for each network update\n",
    "              }\n",
    "\n",
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, device, hyperparams, actor, critic, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an DDPG Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.hyperparams = hyperparams\n",
    "        \n",
    "        self.device = device\n",
    "           \n",
    "        self.BUFFER_SIZE = hyperparams[\"BUFFER_SIZE\"]\n",
    "        self.BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "        self.GAMMA = hyperparams[\"GAMMA\"]\n",
    "        self.TAU = hyperparams[\"TAU\"]\n",
    "        self.LR = hyperparams[\"LR\"]\n",
    "        \n",
    "        self.LEARN_EVERY = hyperparams[\"LEARN_EVERY\"]\n",
    "        self.LEARN_ITERATIONS = hyperparams[\"LEARN_ITERATIONS\"]\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Actor\n",
    "        self.actor = actor(state_size, action_size, seed).to(self.device)\n",
    "        self.actor_target = actor(state_size, action_size, seed).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.LR)\n",
    "        self.actor.eval()\n",
    "       \n",
    "         # Critic\n",
    "        self.critic = critic(state_size, action_size, seed).to(self.device)\n",
    "        self.critic_target = critic(state_size, action_size, seed).to(self.device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.LR)\n",
    "        self.critic.train()\n",
    "        \n",
    "        # Deep Q-Learning Algorithm: Initialize replay memory D with N = BUFFER_SIZE\n",
    "        self.memory = ReplayBuffer(action_size, self.BUFFER_SIZE, self.BATCH_SIZE, seed)\n",
    "       \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "         # Noise process\n",
    "        self.noise = OUNoise(action_size, seed)\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience tuple (S, A, R, S~) in replay memory D\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.LEARN_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in the memory, run learning cycle\n",
    "            if len(self.memory) > self.BATCH_SIZE:\n",
    "                # run learning cycle LEARN_ITERATIONS times \n",
    "                for _ in range(self.LEARN_ITERATIONS):\n",
    "                    # Obtain random minibatch of tuples (S, A, R, S~) from replay memory D\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, self.GAMMA)\n",
    "                    \n",
    "                    # update target networks using soft update\n",
    "                    self.soft_update(self.actor, self.actor_target, self.TAU)   \n",
    "                    self.soft_update(self.critic, self.critic_target, self.TAU)   \n",
    "    \n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        \n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        self.actor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_values = self.actor(states).cpu().data.numpy()\n",
    "        self.actor.train()\n",
    "        \n",
    "        # add noise to the actions to favor exploartion\n",
    "        if add_noise:\n",
    "            action_values += self.noise.sample() / 10.0\n",
    "        \n",
    "        # make sure that resulting value after adding noise is still in [-1, 1] interval\n",
    "        return np.clip(action_values, -1.0, 1.0)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use random minibatch of tuples(S[j], A[j], R[j], S[j+1]) from replay memory D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        with torch.no_grad():            \n",
    "            # get continious actions vector according to the target policy network (Actor)\n",
    "            # for the next states\n",
    "            actions_next = self.actor_target(next_states)\n",
    "            \n",
    "            # evaluate q values for the actions proposed by policy network for the next states\n",
    "            # Q(states_next, actions_next) by target Q network (Critic)\n",
    "            Q_states_next_actions_next = self.critic_target(next_states,  actions_next)\n",
    "                       \n",
    "            # calculate target value\n",
    "            # y[j]= r[j] if episode terminates (done = 1)\n",
    "            y = rewards + gamma * Q_states_next_actions_next * (1.0 - dones)\n",
    "        \n",
    "        # get expected Q value \n",
    "        Q_expected = self.critic(states, actions)\n",
    "        \n",
    "        # critic loss is normal MSE loss, we need to minimize\n",
    "        # difference between actual Q values and target Q values for specific state, action pairs\n",
    "        critic_loss = F.mse_loss(Q_expected, y)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic.parameters(), 1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # predict new actions according to current policy network\n",
    "        predicted_actions = self.actor(states)\n",
    "        \n",
    "        # we need to maximize Q values calculated by critic based on predicted actions,\n",
    "        # so we need to perform gradient ascent (minus sign)\n",
    "        actor_loss = -self.critic(states, predicted_actions).mean()\n",
    "        \n",
    "        # backpropagation\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.noise.reset()\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Setup training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=100, max_t=1000, model_file_actor = 'actor.pth', model_file_critic = 'critic.pth'):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    all_scores = []                     # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)   # last 100 scores\n",
    "    scores_window10 = deque(maxlen=10)  # last 10 scores\n",
    "    max_score = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # Not sure why - sometimes unity environment is not reset after first call.\n",
    "        # so call reset two times - works like a charm\n",
    "        states = env.reset(train_mode=True)\n",
    "        states = env.reset(train_mode=True)\n",
    "        \n",
    "        # reset scores for the episode\n",
    "        scores = np.zeros(states.shape[0], dtype=float)\n",
    "        \n",
    "        # for time step t <- 1 to T:\n",
    "        for t in range(max_t):\n",
    "           \n",
    "            # Choose action from state S using policy network:\n",
    "            actions = agent.act(states)\n",
    "        \n",
    "            # Take an action and observe result\n",
    "            next_states, rewards, dones = env.step(actions)\n",
    "         \n",
    "            # Sample and Learn section\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break \n",
    "      \n",
    "        mean_score = np.mean(scores)\n",
    "        scores_window10.append(mean_score)\n",
    "        scores_window.append(mean_score)       # save most recent score\n",
    "        all_scores.append(mean_score)              # save most recent score\n",
    "        \n",
    "        print('Episode {}\\t Episode score: {:.3f} \\t Mean-10 score: {:.3f} \\t Mean-100 score: {:.3f}'\n",
    "              .format(i_episode, mean_score, np.mean(scores_window10), np.mean(scores_window)))\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score over 100 episodes: {:.3f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "            if np.mean(scores_window) > max_score:\n",
    "                print('Saving model... Episode: {} \\tAverage Score: {:.6f}\\n'.format(i_episode, np.mean(scores_window)))\n",
    "                torch.save(agent.actor.state_dict(), model_file_actor)\n",
    "                torch.save(agent.critic.state_dict(), model_file_critic)\n",
    "                max_score = np.mean(scores_window)\n",
    "        \n",
    "        if np.mean(scores_window) > 30.5:\n",
    "                print('Solved! Saving model... Episode: {} \\tAverage Score: {:.6f}\\n'.format(i_episode, np.mean(scores_window)))\n",
    "                torch.save(agent.actor.state_dict(), model_file_actor)\n",
    "                torch.save(agent.critic.state_dict(), model_file_critic)\n",
    "                max_score = np.mean(scores_window)\n",
    "                break\n",
    "                \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Set device and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "    \n",
    "def moving_average(data_set, periods=3):\n",
    "    weights = np.ones(periods) / periods\n",
    "    return np.convolve(data_set, weights, mode='valid')\n",
    "\n",
    "def plot_comparison(models,  labels, periods):\n",
    "    \n",
    "    fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    \n",
    "    for m, l in zip(models, labels):\n",
    "        moving_m =  moving_average(m, periods)\n",
    "        plt.plot(moving_m, label=l)\n",
    "    \n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.title(\"DQN Models comparison\", fontsize=12, fontweight='bold')\n",
    "    plt.xlabel(\"Eisode #\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\t Episode score: 0.970 \t Mean-10 score: 0.970 \t Mean-100 score: 0.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexeys\\AppData\\Local\\Continuum\\anaconda3\\envs\\drlnd\\lib\\site-packages\\ipykernel_launcher.py:143: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\t Episode score: 0.730 \t Mean-10 score: 0.850 \t Mean-100 score: 0.850\n",
      "Episode 3\t Episode score: 0.800 \t Mean-10 score: 0.833 \t Mean-100 score: 0.833\n",
      "Episode 4\t Episode score: 0.960 \t Mean-10 score: 0.865 \t Mean-100 score: 0.865\n",
      "Episode 5\t Episode score: 1.090 \t Mean-10 score: 0.910 \t Mean-100 score: 0.910\n",
      "Episode 6\t Episode score: 1.170 \t Mean-10 score: 0.953 \t Mean-100 score: 0.953\n",
      "Episode 7\t Episode score: 1.110 \t Mean-10 score: 0.976 \t Mean-100 score: 0.976\n",
      "Episode 8\t Episode score: 1.640 \t Mean-10 score: 1.059 \t Mean-100 score: 1.059\n",
      "Episode 9\t Episode score: 2.170 \t Mean-10 score: 1.182 \t Mean-100 score: 1.182\n",
      "Episode 10\t Episode score: 2.320 \t Mean-10 score: 1.296 \t Mean-100 score: 1.296\n",
      "Episode 11\t Episode score: 0.620 \t Mean-10 score: 1.261 \t Mean-100 score: 1.235\n",
      "Episode 12\t Episode score: 1.500 \t Mean-10 score: 1.338 \t Mean-100 score: 1.257\n",
      "Episode 13\t Episode score: 1.850 \t Mean-10 score: 1.443 \t Mean-100 score: 1.302\n",
      "Episode 14\t Episode score: 1.530 \t Mean-10 score: 1.500 \t Mean-100 score: 1.319\n",
      "Episode 15\t Episode score: 1.770 \t Mean-10 score: 1.568 \t Mean-100 score: 1.349\n",
      "Episode 16\t Episode score: 1.080 \t Mean-10 score: 1.559 \t Mean-100 score: 1.332\n",
      "Episode 17\t Episode score: 0.610 \t Mean-10 score: 1.509 \t Mean-100 score: 1.289\n",
      "Episode 18\t Episode score: 1.350 \t Mean-10 score: 1.480 \t Mean-100 score: 1.293\n",
      "Episode 19\t Episode score: 1.470 \t Mean-10 score: 1.410 \t Mean-100 score: 1.302\n",
      "Episode 20\t Episode score: 2.890 \t Mean-10 score: 1.467 \t Mean-100 score: 1.381\n",
      "Episode 21\t Episode score: 0.500 \t Mean-10 score: 1.455 \t Mean-100 score: 1.340\n",
      "Episode 22\t Episode score: 2.790 \t Mean-10 score: 1.584 \t Mean-100 score: 1.405\n",
      "Episode 23\t Episode score: 0.910 \t Mean-10 score: 1.490 \t Mean-100 score: 1.384\n",
      "Episode 24\t Episode score: 1.990 \t Mean-10 score: 1.536 \t Mean-100 score: 1.409\n",
      "Episode 25\t Episode score: 1.630 \t Mean-10 score: 1.522 \t Mean-100 score: 1.418\n",
      "Episode 26\t Episode score: 1.520 \t Mean-10 score: 1.566 \t Mean-100 score: 1.422\n",
      "Episode 27\t Episode score: 1.530 \t Mean-10 score: 1.658 \t Mean-100 score: 1.426\n",
      "Episode 28\t Episode score: 2.100 \t Mean-10 score: 1.733 \t Mean-100 score: 1.450\n",
      "Episode 29\t Episode score: 1.400 \t Mean-10 score: 1.726 \t Mean-100 score: 1.448\n",
      "Episode 30\t Episode score: 3.440 \t Mean-10 score: 1.781 \t Mean-100 score: 1.515\n",
      "Episode 31\t Episode score: 3.470 \t Mean-10 score: 2.078 \t Mean-100 score: 1.578\n",
      "Episode 32\t Episode score: 4.790 \t Mean-10 score: 2.278 \t Mean-100 score: 1.678\n",
      "Episode 33\t Episode score: 2.360 \t Mean-10 score: 2.423 \t Mean-100 score: 1.699\n",
      "Episode 34\t Episode score: 3.190 \t Mean-10 score: 2.543 \t Mean-100 score: 1.743\n",
      "Episode 35\t Episode score: 1.310 \t Mean-10 score: 2.511 \t Mean-100 score: 1.730\n",
      "Episode 36\t Episode score: 4.740 \t Mean-10 score: 2.833 \t Mean-100 score: 1.814\n",
      "Episode 37\t Episode score: 4.730 \t Mean-10 score: 3.153 \t Mean-100 score: 1.893\n",
      "Episode 38\t Episode score: 3.260 \t Mean-10 score: 3.269 \t Mean-100 score: 1.929\n",
      "Episode 39\t Episode score: 2.880 \t Mean-10 score: 3.417 \t Mean-100 score: 1.953\n",
      "Episode 40\t Episode score: 3.830 \t Mean-10 score: 3.456 \t Mean-100 score: 2.000\n",
      "Episode 41\t Episode score: 1.880 \t Mean-10 score: 3.297 \t Mean-100 score: 1.997\n",
      "Episode 42\t Episode score: 2.530 \t Mean-10 score: 3.071 \t Mean-100 score: 2.010\n",
      "Episode 43\t Episode score: 7.170 \t Mean-10 score: 3.552 \t Mean-100 score: 2.130\n",
      "Episode 44\t Episode score: 2.600 \t Mean-10 score: 3.493 \t Mean-100 score: 2.140\n",
      "Episode 45\t Episode score: 6.130 \t Mean-10 score: 3.975 \t Mean-100 score: 2.229\n",
      "Episode 46\t Episode score: 6.050 \t Mean-10 score: 4.106 \t Mean-100 score: 2.312\n",
      "Episode 47\t Episode score: 7.260 \t Mean-10 score: 4.359 \t Mean-100 score: 2.417\n",
      "Episode 48\t Episode score: 3.750 \t Mean-10 score: 4.408 \t Mean-100 score: 2.445\n",
      "Episode 49\t Episode score: 10.110 \t Mean-10 score: 5.131 \t Mean-100 score: 2.602\n",
      "Episode 50\t Episode score: 6.190 \t Mean-10 score: 5.367 \t Mean-100 score: 2.673\n",
      "Episode 51\t Episode score: 4.920 \t Mean-10 score: 5.671 \t Mean-100 score: 2.717\n",
      "Episode 52\t Episode score: 10.090 \t Mean-10 score: 6.427 \t Mean-100 score: 2.859\n",
      "Episode 53\t Episode score: 8.830 \t Mean-10 score: 6.593 \t Mean-100 score: 2.972\n",
      "Episode 54\t Episode score: 10.520 \t Mean-10 score: 7.385 \t Mean-100 score: 3.112\n",
      "Episode 55\t Episode score: 4.250 \t Mean-10 score: 7.197 \t Mean-100 score: 3.132\n",
      "Episode 56\t Episode score: 8.420 \t Mean-10 score: 7.434 \t Mean-100 score: 3.227\n",
      "Episode 57\t Episode score: 5.270 \t Mean-10 score: 7.235 \t Mean-100 score: 3.263\n",
      "Episode 58\t Episode score: 9.410 \t Mean-10 score: 7.801 \t Mean-100 score: 3.369\n",
      "Episode 59\t Episode score: 6.300 \t Mean-10 score: 7.420 \t Mean-100 score: 3.418\n",
      "Episode 60\t Episode score: 10.540 \t Mean-10 score: 7.855 \t Mean-100 score: 3.537\n",
      "Episode 61\t Episode score: 7.130 \t Mean-10 score: 8.076 \t Mean-100 score: 3.596\n",
      "Episode 62\t Episode score: 4.360 \t Mean-10 score: 7.503 \t Mean-100 score: 3.608\n",
      "Episode 63\t Episode score: 8.780 \t Mean-10 score: 7.498 \t Mean-100 score: 3.690\n",
      "Episode 64\t Episode score: 5.650 \t Mean-10 score: 7.011 \t Mean-100 score: 3.721\n",
      "Episode 65\t Episode score: 7.850 \t Mean-10 score: 7.371 \t Mean-100 score: 3.784\n",
      "Episode 66\t Episode score: 7.480 \t Mean-10 score: 7.277 \t Mean-100 score: 3.840\n",
      "Episode 67\t Episode score: 8.420 \t Mean-10 score: 7.592 \t Mean-100 score: 3.909\n",
      "Episode 68\t Episode score: 7.750 \t Mean-10 score: 7.426 \t Mean-100 score: 3.965\n",
      "Episode 69\t Episode score: 6.190 \t Mean-10 score: 7.415 \t Mean-100 score: 3.998\n",
      "Episode 70\t Episode score: 6.430 \t Mean-10 score: 7.004 \t Mean-100 score: 4.032\n",
      "Episode 71\t Episode score: 9.490 \t Mean-10 score: 7.240 \t Mean-100 score: 4.109\n",
      "Episode 72\t Episode score: 11.460 \t Mean-10 score: 7.950 \t Mean-100 score: 4.211\n",
      "Episode 73\t Episode score: 8.440 \t Mean-10 score: 7.916 \t Mean-100 score: 4.269\n",
      "Episode 74\t Episode score: 3.560 \t Mean-10 score: 7.707 \t Mean-100 score: 4.260\n",
      "Episode 75\t Episode score: 12.800 \t Mean-10 score: 8.202 \t Mean-100 score: 4.373\n",
      "Episode 76\t Episode score: 11.830 \t Mean-10 score: 8.637 \t Mean-100 score: 4.472\n",
      "Episode 77\t Episode score: 6.360 \t Mean-10 score: 8.431 \t Mean-100 score: 4.496\n",
      "Episode 78\t Episode score: 9.520 \t Mean-10 score: 8.608 \t Mean-100 score: 4.561\n",
      "Episode 79\t Episode score: 7.830 \t Mean-10 score: 8.772 \t Mean-100 score: 4.602\n",
      "Episode 80\t Episode score: 8.460 \t Mean-10 score: 8.975 \t Mean-100 score: 4.650\n",
      "Episode 81\t Episode score: 13.490 \t Mean-10 score: 9.375 \t Mean-100 score: 4.759\n",
      "Episode 82\t Episode score: 7.890 \t Mean-10 score: 9.018 \t Mean-100 score: 4.797\n",
      "Episode 83\t Episode score: 20.060 \t Mean-10 score: 10.180 \t Mean-100 score: 4.981\n",
      "Episode 84\t Episode score: 9.950 \t Mean-10 score: 10.819 \t Mean-100 score: 5.040\n",
      "Episode 85\t Episode score: 10.640 \t Mean-10 score: 10.603 \t Mean-100 score: 5.106\n",
      "Episode 86\t Episode score: 14.870 \t Mean-10 score: 10.907 \t Mean-100 score: 5.220\n",
      "Episode 87\t Episode score: 14.700 \t Mean-10 score: 11.741 \t Mean-100 score: 5.329\n",
      "Episode 88\t Episode score: 16.050 \t Mean-10 score: 12.394 \t Mean-100 score: 5.451\n",
      "Episode 89\t Episode score: 17.930 \t Mean-10 score: 13.404 \t Mean-100 score: 5.591\n",
      "Episode 90\t Episode score: 10.060 \t Mean-10 score: 13.564 \t Mean-100 score: 5.641\n",
      "Episode 91\t Episode score: 14.530 \t Mean-10 score: 13.668 \t Mean-100 score: 5.738\n",
      "Episode 92\t Episode score: 11.720 \t Mean-10 score: 14.051 \t Mean-100 score: 5.803\n",
      "Episode 93\t Episode score: 14.830 \t Mean-10 score: 13.528 \t Mean-100 score: 5.900\n",
      "Episode 94\t Episode score: 15.230 \t Mean-10 score: 14.056 \t Mean-100 score: 6.000\n",
      "Episode 95\t Episode score: 15.750 \t Mean-10 score: 14.567 \t Mean-100 score: 6.102\n",
      "Episode 96\t Episode score: 20.110 \t Mean-10 score: 15.091 \t Mean-100 score: 6.248\n",
      "Episode 97\t Episode score: 15.950 \t Mean-10 score: 15.216 \t Mean-100 score: 6.348\n",
      "Episode 98\t Episode score: 16.670 \t Mean-10 score: 15.278 \t Mean-100 score: 6.453\n",
      "Episode 99\t Episode score: 10.680 \t Mean-10 score: 14.553 \t Mean-100 score: 6.496\n",
      "Episode 100\t Episode score: 18.850 \t Mean-10 score: 15.432 \t Mean-100 score: 6.620\n",
      "Episode 100\tAverage Score over 100 episodes: 6.620\n",
      "Saving model... Episode: 100 \tAverage Score: 6.619700\n",
      "\n",
      "Episode 101\t Episode score: 15.230 \t Mean-10 score: 15.502 \t Mean-100 score: 6.762\n",
      "Episode 102\t Episode score: 12.930 \t Mean-10 score: 15.623 \t Mean-100 score: 6.884\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-43d0c34e6c64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m222\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file_actor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DDPG_actor.pth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file_critic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DDPG_critic.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DDPGScores.npy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-3f7d194f7b59>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, n_episodes, max_t, model_file_actor, model_file_critic)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# Sample and Learn section\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-9c115f281360>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     77\u001b[0m                     \u001b[1;31m# Obtain random minibatch of tuples (S, A, R, S~) from replay memory D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                     \u001b[1;31m# update target networks using soft update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-9c115f281360>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;31m# predict new actions according to current policy network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mpredicted_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;31m# we need to maximize Q values calculated by critic based on predicted actions,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-ad4503ebff56>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;34m\"\"\"Build a network that maps state -> action values.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Layer 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_norm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1352\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1353\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparams = { \"BUFFER_SIZE\" : int(1e6),  # replay buffer size\n",
    "                \"BATCH_SIZE\" : 1024,         # minibatch size\n",
    "                \"GAMMA\" : 0.99,             # discount factor\n",
    "                \"TAU\" : 1e-3,               # for soft update of target parameters\n",
    "                \"LR\" : 1e-4,                # learning rate \n",
    "                \"LEARN_EVERY\" : 10,         # how often to update the network\n",
    "                \"LEARN_ITERATIONS\" : 10,    # how many iterations needed for each network update\n",
    "              }\n",
    "\n",
    "agent = DDPGAgent(device, hyperparams, Actor, Critic, state_size=env.state_size, action_size=env.action_size, seed=222)\n",
    "\n",
    "scores = train(env, agent, n_episodes=500, max_t=1000, model_file_actor = 'DDPG_actor.pth', model_file_critic = 'DDPG_critic.pth')\n",
    "\n",
    "np.save(\"DDPGScores.npy\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "aver = moving_average(scores, 10)\n",
    "\n",
    "plot_scores(aver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Load agent and watch performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6399999856948853\n"
     ]
    }
   ],
   "source": [
    "hyperparams = { \"BUFFER_SIZE\" : int(1e6),  # replay buffer size\n",
    "                \"BATCH_SIZE\" : 1024,         # minibatch size\n",
    "                \"GAMMA\" : 0.99,             # discount factor\n",
    "                \"TAU\" : 1e-3,               # for soft update of target parameters\n",
    "                \"LR\" : 1e-4,                # learning rate \n",
    "                \"LEARN_EVERY\" : 10,         # how often to update the network\n",
    "                \"LEARN_ITERATIONS\" : 10,    # how many iterations needed for each network update\n",
    "              }\n",
    "\n",
    "agent = DDPGAgent(device, hyperparams, Actor, Critic, state_size=env.state_size, action_size=env.action_size, seed=0)\n",
    "\n",
    "actor_state_dict = torch.load('DDPG_actor.pth')\n",
    "critic_state_dict = torch.load('DDPG_critic.pth')\n",
    "\n",
    "agent.actor.load_state_dict(actor_state_dict)\n",
    "agent.actor_target.load_state_dict(actor_state_dict)\n",
    "\n",
    "agent.critic.load_state_dict(critic_state_dict)\n",
    "agent.critic_target.load_state_dict(critic_state_dict)\n",
    "\n",
    "states = env.reset(False)\n",
    "score = 0                \n",
    "\n",
    "while True:\n",
    "    actions = agent.act(states, False)                \n",
    "    next_states, rewards, dones = env.step(actions)            \n",
    "    states = next_states  \n",
    "    score += np.mean(rewards) \n",
    "    if np.any(dones):                                     \n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
